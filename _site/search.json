[
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "The blog posts on this website are all released under a Creative Commons Attribution 4.0 International License. Please feel free to use and share anything you find valuable in these posts, but please cite me too!"
  },
  {
    "objectID": "blog/sorte_apostas/index.html",
    "href": "blog/sorte_apostas/index.html",
    "title": "Sorte e Apostas",
    "section": "",
    "text": "Aqui voce pode encontrar a apresenta√ß√£o: Link da Apresenta√ß√£o\n\n\n\n\n\n\n\n\nYou can also reproduce this by choosing the city you want.\nCheck out the complete code on my Github.\n\n\n\n\n\n\nHey! üëã, did you find my work useful? Consider buying me a coffee ‚òï, by clicking here üëáüèª\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationFor attribution, please cite this work as:\nCosta, Henrique. 2024. ‚ÄúSorte e Apostas.‚Äù April 16, 2024.\nhttps://doi.org/10.59350/s9btn-bfx74."
  },
  {
    "objectID": "blog/regression/index.html",
    "href": "blog/regression/index.html",
    "title": "Trivia Series: Regression",
    "section": "",
    "text": "Rambling (proxy for an introduction)\nWe are experiencing the chaos of the information age, and its technological advents with voracious evolution and expansion. And in the midst of all this we have the advent of Big Data that has reached the delights of many with Data Science, Machine Learning and Artificial Intelligence.\nAs a result, the idea of mathematical and statistical ‚Äúmodels‚Äù became the rage, gaining visibility. And among these models we have the most trivial of all: regression.\nHave you ever stopped to think about where this term comes from, and what it means in practice? Exactly what you thought: ‚ÄúThat‚Äôs the basics!‚Äù That‚Äôs why this series of stories was called ‚Äútrivialities‚Äù, as they are trivialities that sometimes need to be known.\nOn my journey as a Quant backpacker (which is not that long, but long enough to make these observations) I realized that very simple points or concepts are not known to the overwhelming majority, and that is why I observed a lot of misapplication or erroneous judgment in statistical metrics , and I decided to start writing about it.\nAs I have academic roots, I brought the questions that will direct this text:\n\nWhat is regression?\nWhat does this term mean?\nWhere and when did it appear?\nHow to apply?\nAm I applying it correctly?\n\nAnd as usual, I‚Äôll start from the beginning (üòÑüòÑüòÑ).\n\n\nThe beginnings of the Regression\nWhen we think of this term, the idea of linear statistical regression immediately comes to mind. And yes, that‚Äôs right, the term is the same, but why regression? When I think about the meaning of the word, and obviously I already looked it up in a dictionary on the internet, which is the same as going back, regression comes from regressing, returning to a state that has already been overcome or that has evolved but returned to its previous levels. previous ones. It‚Äôs like climbing a ladder but going down backwards.\nAnd that‚Äôs right (üòÑ), regression is twisting, returning to the previous state. This term was coined in 1875 by an amateur mathematician named Francis Galton, who in some books says he was a first cousin of the famous Charles Darwin. Galton brought to the public the coining of this term, calling it ‚Äúregression to the mean‚Äù.\nWhy regression to the mean? Did something that moved end up returning to the mean? Yes that‚Äôs right. The concept only makes sense when we connect it with classical statistics, or better, with the central limit theorem.\nThis theorem is a right arm of probability theory, and builds the statement that when the sample size increases, the sampling distribution gets closer and closer to a normal distribution, and this is fundamental in statistical inference. Translating into simpler language: it‚Äôs like saying that when we observe nature, we perceive a natural state or a standard, normal state of things.\nAn example using Brazil, a country with a tropical climate, it is correct to say that the natural state of the climate is heat, as it is sunny many more days a year than cold or rainy, therefore, on average it is hotter, so the normal of Brazil is to have sunny days, but when it rains we can activate the idea of normality, in which the rain can soon stop and the sun and high temperatures return to the scene.\nRegression to the mean is exactly that, even if you have days with rain, the ‚Äúnormal‚Äù is that there are more sunny days, so rainy days are deviations between the averages, as the average is sunny days. But the term became known even after Galton published a study in 1885 in which he demonstrated, through regression calculations, that the height of children does not tend to reflect the height of their parents, but rather tends to regress towards the population average.\nIt is possible to see how the term gained scale after use by Galton, using the Google tool: Ngram book viewer.\n\n\n\nThe use of regression\nThe application of this method is known to many people, and can be found in different professional areas. But as I come from economic sciences, I will use this line to continue the text.\nThe economic sciences have econometrics as their quantitative tool, which is the metric for analyzing economic theories. In simpler language, it is the use of classical statistical techniques to analyze and test an economic theory. Economists use a lot of regression in econometrics.\nAnd studying econometrics I learned a lot about the fundamentals of regression analysis. But a few paragraphs above I wrote that regression is returning to your previous state, so how would economists use a matrix like that? For the simple fact that the economy works with the assumption of a state of general equilibrium of things.\nThe economist has a single objective, and amazingly, it is not to save money, it is to allocate scarce resources to meet unlimited needs. But how does he do this? Artificial intelligence!!! (üòÑüòÑüòÑ).\nJoking apart. The economist uses optimization as his beacon. Allocating scarce resources to satisfy unlimited needs is only possible by optimizing. And what does optimizing have to do with regression? All! Economists‚Äô assumption of a general equilibrium is only possible using optimization, minimizing costs and maximizing profits, and regressing to the mean or returning to a normal state only reinforces or supports the idea of general equilibrium.\nSo, I will bring a brief example of application, coming from a study material for economists, the book on basic econometrics by Damodar N. Gujarati and Dawn C. Porter, in which we find an application that is well known among economists: the hypothesis of marginal propensity to consume, or MPC.\nKeynes stated: The fundamental psychological law [‚Ä¶] is that men [women] are disposed, as a rule and on an average, to increase their consumption as their income increases, but not in the same proportion as the increase in income. (Keynes, John Maynard. The general theory of employment, interest and money. New York: Harcourt Brace Jovanovich, 1936. p.¬†96.).\nSo to simplify, we will use an econometric model based on a simple linear regression, but first I need to comment that there is a rationale behind a linear regression, which is the central objective of this text, to talk about some basic points that sometimes fall into oblivion .\nAn econometric model (statistical/mathematical) aims to represent the reality we are interested in investigating. The model must be able to capture the relationships between reality and theory, so that the theory can be tested. However, this representation of reality is not complete, a model alone is not capable of representing or describing reality as a whole, so it needs to be conditioned and sometimes restricted.\nKeynes‚Äô hypothesis brings exactly this idea, with the construction of a consumption function, and without many details, his hypothesis of a relationship between consumption and income seems deterministic or an exact relationship. So, as the model cannot have all the information, it uses the most likely or available information and relies on the premise that the other effects are constant or unchanged, not causing direct influence: ceteris paribus.\nI wrote a text about how I see the ceteris paribus condition, and how it helps us understand real-life phenomena. So, returning to the example of Keynes‚Äôs hypothesis, he uses ceteris paribus in an intuitive and implicit way, leaving no details, but establishing the premises: ‚Äúto increase your consumption as your income increases‚Äù. So we already have the relationship we need to build our model.\nIn short, Keynes postulated that the population had a tendency to increase its consumption when its income increases, which was labeled marginal propensity to consume (PMC), and it would be analyzed quantitatively as a rate of variation in consumption, and that this variation would be given in units (say, one dollar) of income, and that it will always be greater than zero, but less than 1, as his observations showed that additional consumption did not have the same level as additional income, that is, people did not they consumed everything they earned and that is why it has to be less than 1.\n\n\nSpecifying the hypothesis-based econometric model\nAlthough Keynes established an apparently positive relationship between income and consumption, he did not stipulate how this relationship happens. To simplify, let‚Äôs use ‚Äúpoetic license‚Äù and suggest the following functional form for the relationship between income and consumption established by Keynes:\n\\[ Y = \\alpha + \\beta X \\;\\;\\;\\;\\;\\;\\;\\; 0 &lt; \\beta &lt; 1 \\]\nwhere \\(Y\\) represents consumption expenditure and \\(X\\) represents income, and \\(\\alpha\\) and \\(\\beta\\) are the model parameters. These parameters represent the effects generated through the relationship between \\(X\\) and \\(Y\\). Sometimes known as marginal effects.\nThe parameter \\(\\alpha\\) is known as ‚Äúintecept‚Äù, or average value. Generally it demonstrates that where the relationship between the variables between \\(X\\) and \\(Y\\) begins. It is the average value, if the marginal effect is null (zero). It‚Äôs like saying that, if \\(X\\) has no influence on \\(Y\\), then when \\(X\\) and \\(Y\\) are related, it is \\(\\alpha\\) that shows the level of this relationship.\nThe parameter \\(\\beta\\) is known as ‚Äúangle‚Äù or ‚Äúangular coefficient‚Äù, also called the slope of a straight line, it determines the slope of a straight line. A straight line because the relationship between \\(X\\) and \\(Y\\) is direct (linear). The angular coefficient is a number that is related to the angle formed between the straight line and the horizontal, describing the slope of the straight line. And when we connect the terms ‚Äúslope‚Äù and ‚Äúangle‚Äù, we can remember the concept of derivation, and that is exactly it, it is the effect of the derivation in relation to \\(X\\). And we know that in economics the concept of marginality is directly correlated to the concept of derivation, which is why the angular coefficient is known as marginal effect.\nAnd the angular coefficient (\\(\\beta\\)) will be our indicator of the relationship between income and consumption, which we will call Marginal Propensity to Consume. This is already constructed by Keynes‚Äô theory, but we are here trying to work on an idea.\nAs I mentioned in the paragraphs above, this relationship does not represent a pure and exact reality. Of course, there are many variables that can affect consumption in addition to income, and therefore the model needs an additional specification, which for me is the charm of the linear regression model.To take into account the influences of other variables that were not imposed in this model:\n\\[ Y = \\alpha + \\beta X + u \\]\nwhere \\(u\\) is known as the disturbance, or error term of the model. It is a random (stochastic) variable that has probabilistic properties. The error term \\(u\\) is intended to represent all other factors that affect consumption but are not explicitly imposed here.\nThe error term is essentially the ceteris paribus condition. It contains all the other effects that interact with Consumption, but they are being nullified, or kept constant/unchanged, that is, there is no variation, so no effect can be felt.\nThat is, by isolating the effect of income on consumption, we artificially create a kind of general equilibrium, showing that the relationship between \\(X\\) and \\(Y\\) is balanced (normal), and that other factors have no influence.\n\n\nEstimating the model\nWith data from Table I.1 of the Gujarati basic econometrics book, which refers to the United States economy in the period 1960-2005. Table I.1 uses aggregate consumption to represent model consumption, and GDP to represent income, and thus we will estimate the marginal propensity to consume. With the data in hand, it‚Äôs time to go to RStudio and start ‚Äúthe work‚Äù. Let‚Äôs Code!\n\n\nShow the code\n# install.packages(\"janitor\")\n# remotes::install_github(\"brunoruas2/gujarati\")\n\n# load libraries\nlibrary(tidyverse)\nlibrary(gujarati)\n# library(janitor)\n\nTableI_1 %&gt;% \n  mutate(\n    Year = str_replace_all(Year, \" \", \"\"),\n    PCE.Y. = str_replace_all(PCE.Y., \" \", \"\"),\n    GDP.X. = str_replace_all(GDP.X., \" \", \"\"),\n  ) %&gt;% \n  as_tibble(.name_repair = janitor::make_clean_names) %&gt;% \n  mutate(\n    pce_y = as.numeric(pce_y),\n    gdp_x = as.numeric(gdp_x)\n  ) -&gt; tbl_pmc\n\n\n\nmodel &lt;- lm(pce_y ~ gdp_x, data = tbl_pmc)\n\n\nmodel %&gt;% \n  summary() %&gt;% \n  pander::pander()\n\n\n\n\n\n\n\n\n\n\n\n\n¬†\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n-299.6\n28.76\n-10.42\n1.877e-13\n\n\ngdp_x\n0.7218\n0.004423\n163.2\n7.244e-63\n\n\n\n\nFitting linear model: pce_y ~ gdp_x\n\n\n\n\n\n\n\n\nObservations\nResidual Std. Error\n\\(R^2\\)\nAdjusted \\(R^2\\)\n\n\n\n\n46\n73.57\n0.9984\n0.9983\n\n\n\n\n\nNow that we have an output from the experimental model, we can obtain numerical estimates of the parameters that will provide us with an empirical resolution for the consumption function proposed here in this example.\nNow that we have an output from the experimental model, we can obtain numerical estimates of the parameters that will provide us with an empirical resolution for the consumption function proposed here in this example.\nWe can note that the statistical technique of regression analysis is the main tool for obtaining such estimates. After running the model, we obtain the following estimates:\n\n\\(\\hat{\\alpha}\\) = -299.6\n\\(\\hat{\\beta}\\) = 0.7218\n\nNow we can empirically translate the estimated consumption function as:\n\\[ \\hat{Y_t} = \\hat{\\alpha} + \\hat{\\beta} X_t \\]\n\\[ \\hat{Y_t} = -299,5913 + 0,7218 X_t \\]\nThe caret above the Y and the parameters \\(\\alpha\\) and \\(\\beta\\) indicates that this is an estimate. The following graph shows this estimated consumption function (i.e., the regression line).\n\n\nShow the code\n# load libraries\nlibrary(tidyverse)\n\n\n\n# building a plot with ggplot2 based on grammar of graphics:\n\ntbl_pmc %&gt;% \nggplot(aes(\n  x = gdp_x, \n  y = pce_y\n  )) +\n  geom_point(aes(\n    color = \"red\", \n    size = .7, \n    alpha = .8\n    )) +\n  geom_smooth(\n    method = \"lm\", \n    se = TRUE,  \n    inherit.aes = TRUE\n    ) +\n  theme(legend.position = \"none\") +\n  labs(\n    title = \"Econometric model of the Keynesian consumption function\",\n    subtitle = \"Values for 1960-2005 in billions of 2000 dollars\",\n    x = \"Gross Domestic Product\",\n    y = \"Personal Consumption Expenditures\",\n    caption = \"Source: Economics Report of the President, 2007, Table B-2, p, 230.\")\n\n\n\n\n\n\n\n\n\nThe red dots represent our income data (GDP), and the blue line represents the regression line.\nAs we can see in the model graph, it is possible to say that the regression line (blue) fits the data well, this means that the points (red) on the graph that represent the data are very close to the regression line.\nThe table of model results and the graph show us that, in this analyzed period (1960-2005), a marginal effect of income (GDP) on consumption behavior of almost 0.72 is estimated.\nAs I already mentioned, this marginal effect estimate comes from the angular coefficient, which we will now call ‚ÄúCoefficient of the Marginal Propensity to Consume‚Äù, and this coefficient is telling us that, in the sampled period, whenever there is an increase of one dollar in income In real terms, on average, there is an additional increase of around 72 cents in real consumption expenditure, that is: for every 1 dollar more, there is, on average, a marginal effect of 72 cents. If it increases by 1 dollar, consumption increases by 72 cents, and the opposite is also true.\n\n\n\n\n\n\nAverage Marginal Effects\n\n\n\n\n\nAttention to a critical point: we use the term ‚Äúaverage‚Äù or we always use the phrase ‚Äúon average‚Äù because the relationship between consumption and income is inexact, that is, it is not a direct causal relationship; and this is very clear in the model graph, as not all red points in the data are exactly on the regression line.\nIn general terms, we can say that, ‚Äú[‚Ä¶]according to our data, average consumer spending increases by about 70 cents for every dollar increase in income.\nAnd this reinforces the idea of normality and average. We use the average as the optimal moment, because under normal conditions of temperature and pressure the effect is recognized in the average, because no matter the changes or oscillations, if we believe in the normality that after these variations everything returns to normal, then the average is the central point, that is, the normal.\nAs I tried to explain before, what we call marginal effects are partial derivatives of the regression equation in relation to each variable in the model; So therefore, the average marginal effects are simply the average of these partial derivatives. In ordinary least squares regression, the estimated slope coefficients (angular and/or \\(\\hat \\beta\\)) are marginal effects.\n\n\n\nNow we know that on the surface, Keynes‚Äô assumption appears to make sense. We found this positive relationship between income and consumption. However, we still cannot simply accept this value as a confirmation of the Keynesian consumption hypothesis, as it is not enough to just find the effect, it is necessary to test it, and that is science. And following the scientific standard, let‚Äôs create a problem question: ‚ÄúIs this estimate sufficiently below unity?‚Äù This is to convince us that it is not a result due to chance or a peculiar insight that the data we use is showing.\n\n\nTesting a hypothesis\nBased on the idea of a scientific test, in which the statistical framework is used as a tool, is the marginal effect of 0.72 statistically smaller than 1? If this is true, it will be support for the birth of a theory. This practice of testing, or better said, corroborating or refuting existing economic theories or those that are emerging, based on sample evidence, which is basically what we have just done, has a foundation based on statistical theory, in a field of study known as statistical inference (hypothesis testing).\nLet‚Äôs consider that the adjusted model we have just put together is a reasonably good approximation of reality, and that is one of the objectives of a model, but we need to be critical and criticize our own model, trying to understand if the estimates obtained are in accordance with the expectations of the theory being tested, which in this case is Keynes‚Äô theory of marginal propensity to consume.\nEconomist Milton Friedman said that a theory or hypothesis that is not verifiable with empirical evidence may not be admissible as part of scientific research. Whereas Keynes expected the marginal propensity to consume to be positive but less than 1. In our model, we estimate a marginal propensity of about 0.72. But before this figure can be accepted as a confirmation of Keynesian consumption theory, we need to statistically test this estimate to convince ourselves that it is not a result due to chance or a peculiarity of the data we use.\nPara isso, iremos utilizar do conceito de teste de hipotese. Nesse teste teremos que lidar com dois tipos de erros poss√≠veis de ocorrer:\n\nA hip√≥tese nula √© rejeitada embora seja verdadeira (erro tipo I)\nA hip√≥tese nula n√£o √© rejeitada embora seja falsa (erro tipo II)\n\n\n\n\n\n\n\nTip with Title\n\n\n\nThe significance level of the test is the probability to commit a type-I-error we are willing to accept in advance. E.g., using a prespecified significance level of 0.05, we reject the null hypothesis if and only if the p-value is less than 0.05. The significance level is chosen before the test is conducted.\nAn equivalent procedure is to reject the null hypothesis if the observed test statistic is, in absolute value terms, larger than the critical value of the test statistic. The critical value is determined by the significance level chosen and defines two disjoint sets of values which are called acceptance region and rejection region. The acceptance region contains all values of the test statistic for which the test does not reject while the rejection region contains all the values for which the test does reject.\nThe p-value is the probability that, in repeated sampling under the same conditions a test statistic is observed that provides just as much evidence against the null hypothesis as the test statistic actually observed.\nThe actual probability that the test rejects the true null hypothesis is called the size of the test. In an ideal setting, the size does equal the significance level.\nThe probability that the test correctly rejects a false null hypothesis is called power.\nHanck, Christoph, Martin Arnold, Alexander Gerber, and Martin Schmelzer. Introduction to Econometrics with R. Universit√§t Duisburg-Essen, 2021.\n\n\nYou can also reproduce this by choosing the city you want.\nCheck out the complete code on my Github.\n\n\n\n\n\n\nHey! üëã, did you find my work useful? Consider buying me a coffee ‚òï, by clicking here üëáüèª\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationFor attribution, please cite this work as:\nCosta, Henrique. 2023. ‚ÄúTrivia Series: Regression.‚Äù\nDecember 20, 2023. https://doi.org/10.59350/xs70x-mar87."
  },
  {
    "objectID": "blog/desafio_EconViz_01/index.html",
    "href": "blog/desafio_EconViz_01/index.html",
    "title": "Reproduzindo gr√°fico do Economista Visual - Valor da cesta b√°sica (PT-BR)",
    "section": "",
    "text": "Na publica√ß√£o de hoje, quero trazer uma brincadeira que fiz comigo mesmo, um pequeno incentivo para voltar a publicar conte√∫dos sobre R (eu j√° estava enferrujado) entre outros assunto sobre economia.\nPasseando pelo instagram, encontrei um perfil que p√∫blica muita informa√ß√£o sobre economia em formato de visualiza√ß√£o, o Economista Visual.\nVendo um gr√°fico sobre o pre√ßo da cesta b√°sica na cidade de S√£o Paulo - SP\n\nResolvi criar o desafio de reproduzir esta visualiza√ß√£o. Claro que n√£o conseguiria deixar igual, mas poderia chegar bem perto.\nVamos come√ßar pela fonte dos dados. Os dados utilizado no gr√°fico vem da base do DIEESE (Departamento Intersindical de Estat√≠stica e Estudos Socioecon√¥micos), que faz o levantamento do valor da Cesta B√°sica de Alimentos em v√°rias cidades do pa√≠s.\nAcessando o website do DIEESE, na pagina do banco de dados de Cesta B√°sica de Alimentos √© poss√≠vel escolher a cidade desejada e fazer o download da planilha contendo as informa√ß√µes sobre o valor da cesta b√°sica.\n\nCom os dados em m√£os, √© hora de ir para o RStudio e come√ßar ‚Äúos trabalhos‚Äù. Let‚Äôs Code!\n# para retirar notacoes cientifica\noptions(scipen = 999)\n\n\n# Pacotes necessarios para o projeto\n\nlibrary(tidyverse)     # conjunto de pacotes essenciais\nlibrary(readxl)        # pacote para \"ler\" arquivos excel\nlibrary(lubridate)     # pacote para trabalhar com datas\nlibrary(scales)        # pacote para trabalhar com escalas\nlibrary(here)          # fluxos de trabalho orientados a projetos\nParticularmente, prefiro utilizar um fluxo orientado a projetos e por isso o pacote hereest√° na lista acima. O pacote here implementa uma maneira mais simples de encontrar seus arquivos. O objetivo do pacote here √© permitir a refer√™ncia f√°cil de arquivos em fluxos de trabalho orientados a projetos. Em contraste com o uso setwd(), que √© fr√°gil e dependente da maneira como voc√™ organiza seus arquivos, here usa o diret√≥rio de n√≠vel superior de um projeto para construir facilmente caminhos para os arquivos.\nAp√≥s carregar os pacotes, √© hora de ler os dados que est√£o na planilha em formato .xls.\n# Para carregar os dados usaremos o codigo a seguir:\n\ncesta_basica &lt;- read_excel(     # funcao que ler arquivos .xls\n  here::here(                   # funcao que informa o local\n    \"dados\",\"exporta.xls\"       # \"local\", \"arquivo\"\n    )\n)\nAgora que os dados j√° est√£o no global environment, vamos usar de alguns ‚Äúcostumes‚Äù de pr√©-analise para conhecer os dados. Lembrando que qualquer modifica√ß√£o nos dados ser√° feito inteiramente e diretamente no R.\n# visualizar a situacao dos dados\nhead(cesta_basica)              # funcao imprime no console \n                                # as 10 primeiras observacoes\n\n## A tibble: 6 x 2\n#  `Gasto Mensal - Total da Cesta` ...2     \n#  &lt;chr&gt;                           &lt;chr&gt;    \n# 1 NA                              S√£o Paulo\n# 2 01-2000                         112.22   \n# 3 02-2000                         110.8    \n# 4 03-2000                         115.13   \n# 5 04-2000                         115.92   \n# 6 05-2000                         111.78 \nO resultado mostra o √≥bvio, que os dados precisam de ‚Äútratamento‚Äù antes de pensarmos em extrair qualquer informa√ß√£o. Aqui neste caso, precisa retirar a primeira linha, que cont√©m NA (not available) e uma palavra (S√£o Paulo) onde deveria ser n√∫meros. Os nomes das colunas (vari√°veis) tamb√©m necessitam de aten√ß√£o. Vamos olhar um pouco mais, com a fun√ß√£o tail().\ntail(cesta_basica)              # funcao imprime no console \n                                # as 10 ultimas observacoes\n\n# A tibble: 6 x 2\n#  `Gasto Mensal - Total da Cesta`    ...2          \n#   &lt;chr&gt;                              &lt;chr&gt;         \n# 1 11-2020                            629.179999999~\n# 2 12-2020                            631.460000000~\n# 3 01-2021                            654.149999999~\n# 4 Fonte: DIEESE                      NA            \n# 5 (1) S√©rie recalculada, conforme mudan√ßa metodol√≥gica realizada # na ~ NA            \n# 6 Tomada especial de pre√ßos a partir de abril de 2020 NA    \nComo esperado, s√≥ refor√ßa a necessidade de um tratamento simples nos dados. A priori √© uma ‚Äúlimpeza‚Äù de linhas indesejadas.\nPor boas pr√°ticas, irei modificar o ‚Äúnome‚Äù do objeto (data frame), pois CB √© mais simples e f√°cil que cesta_basica.\nCB &lt;- cesta_basica %&gt;%          # criando um objeto reduzindo\n                                # o nome do dataframe para CB\n                                # ao inves de cesta_basica\n                                # e usando o operador \" %&gt;% \"\n  \n  rename(                       # funcao para renomear colunas\n    Data = `Gasto Mensal - Total da Cesta`,\n    Valor = ...2\n  ) %&gt;%                       \n  slice(2:255)                  # funcao que corta os dados\n                                # retirando apenas o desejado\nAgora temos que verificar as caracter√≠sticas dos nossos dados. Utilizo a fun√ß√£o srt().\nstr(CB)\n\n# tibble [254 x 2] (S3: tbl_df/tbl/data.frame)\n# $ Data : chr [1:254] \"01-2000\" \"02-2000\" \"03-2000\" \"04-2000\" ...\n# $ Valor: chr [1:254] \"112.22\" \"110.8\" \"115.13\" \"115.92\" ...\nTemos 254 observacoes e 2 colunas (variaveis) as 2 variaveis estao em formato -chr- caracteres e precisamos modificar esses formatos, passando a variavel Valor para o formato n√∫merico, e a vari√°vel Data como datas, para isso utilizaremos os passos a seguir:\nCB &lt;- CB %&gt;%                   # substituindo o dataframe\n  mutate(                      # funcao p/ modificar variaveis\n    Data = rep(seq(            # Substituindo a variavel Data\n                               # por uma variavel no formato de datas\n                               # criando uma sequencia de datas\n      from = as.Date(\"2000-01-01\"),\n      to = as.Date(\"2021-01-01\"),\n      by =\"1 month\"\n    )),\n    Valor = as.numeric(Valor) # modificando a variavel ¬¥Valor¬¥\n                              # para o formato numerico\n  )\n\n# Erro: Problem with `mutate()` input `Data`.\n# x Input `Data` can't be recycled to size 254.\n# i Input `Data` is `rep(...)`.\n# i Input `Data` must be size 254 or 1, not 253.\nPercebemos esse erro que significa que o vetor de datas que tentamos criar nao √© do mesmo ‚Äútamanho‚Äù que a variavel Data anterior. A variavel Data tem 254 observacoes e nosso vetor de datas tem apenas 253. Totalmente sem sentido, pois criamos um vetor de datas que segue uma sequencia mes a mes de 01/01/2000 ate 01/01/2021.\nA priori, temos a hip√≥tese de que h√° valores e data repetidas como a base de dados eh pequena (254 obs) usaremos a funcao View() para ver a base completa e entende-la para encontrar a observacao repetida.\nEncontramos a observacao 193, com a data 12-2015 (1) e valor de 418.13. Como nao pesquisei a fundo o motivo, vou considerar como um reajuste no valor, entao irei retirar a observacao anterior (192).\nVou utilizar novamente o codigo acima, por√©m com um upgrade, adicionarei a funcao slice() para recortar a observacao 192.\nCB &lt;- CB %&gt;%                   # substituindo o dataframe\n  slice(-192) %&gt;%              # funcao p/ recortar \n  mutate(                      # funcao p/ modificar variaveis\n    Data = rep(seq(            # Substituindo a variavel Data\n                               # por uma variavel no formato de dadtas\n                               # criando uma sequencia de datas\n      from = as.Date(\"2000-01-01\"), \n      to = as.Date(\"2021-01-01\"), \n      by =\"1 month\"\n    )),\n    Valor = as.numeric(Valor)  # modificando a variavel ¬¥Valor¬¥\n                               # para o formato numerico\n  )\nAgora vamos para a parte mais delicada de todo o desafio: criar a visualiza√ß√£o, ou seja, reproduzir um gr√°fico parecido com o gr√°fico que foi publicado pelo Economista Visual.\nAntes de construir o gr√°fico, vamos elaborar a customiza√ß√£o de uma tema para aplicar juntamente com o pacote ggplot2. Esta customiza√ß√£o nos ajudar√° a gerar um gr√°fico parecido.\nNa constru√ß√£o do tema, precisamos usar a mesma fonte que o Economista Visual usa em seu gr√°fico, mas preferir n√£o investigar qual fonte √© usada, e usei uma fonte qualquer. Decidir usar a fonte Teko, que pode ser encontrada no Google Fonts\ncustom_theme &lt;- function(){\n  font &lt;- \"Teko\"             # definindo a fonte a ser utilizada\n  theme(\n    # Elementos de grade e painel\n    panel.border =  element_blank(),       # sem borda\n    panel.grid.major.x = element_blank(),  # sem grades em X\n    panel.grid.minor.x = element_blank(),  # sem grades em X\n    panel.grid.major.y = element_line(\n    color = \"#d2d2d2\"                      # cor para a linha\n    ),                                     # da grade em Y\n    panel.grid.minor.y = element_blank(),  # sem grades menor em Y,     \n    axis.ticks = element_blank(),          # tira pontos do eixo\n    # Elementos textuais\n    plot.title = element_text(             # Titulo\n      family = font,                       # definir font\n      size = 20,                           # definir tamanho\n      face = 'bold',                       # negrito\n      color = \"black\",                     # cor da fonte\n      hjust = 0,                           # ajuste p/ esquerda*\n      vjust = 2),                          \n    \n    # *O valor de hjust e vjust sao \n    # definidos entre 0 e 1:\n    # 0 significa justificado a esquerda\n    # 1 significa justificado a direita\n    # 0.5 significa justificado ao meio\n    \n    plot.subtitle = element_text(          # Sub-titulo\n      family = font,                       # fonte\n      color=\"black\",                       # cor da fonte\n      size = 12),                          # tamanho\n      \n    plot.caption = element_text(           # Legenda\n      family = font,                       # fonte\n      size = 9,                            # tamanho da fonte\n      face = \"italic\",                     # italico\n      colour = \"#4c4c4c\",                  # cor da fonte\n      hjust = 0),                          # ajuste a esquerda\n    \n    axis.title = element_text(             # Titulo dos eixos\n      family = font,                       # fonte\n      face = 'bold',                       # negrito\n      color = \"#2e2e2e\",                   # cor da fonte\n      size = 10),                          # tamanho\n    \n    axis.text = element_text(              # Texto dos eixos\n      family = font,                       # fonte\n      color = \"#2e2e2e\",                   # cor\n      size = 9),                           # tamanho\n    \n    axis.text.x = element_text(            # margem p/ texto dos eixos\n      color = \"#2e2e2e\",                   # cor\n      margin=margin(5, b = 10)),\n    \n    axis.text.y = element_text(            # margem p/ texto dos eixos\n      color = \"#2e2e2e\",                   # cor\n      margin=margin(10, b = 20)),\n    \n    legend.position=\"bottom\",              # Posi√ß√£o da legenda\n                                           # bottom = meio-inferior\n    legend.title = element_blank(),        # Anular o t√≠tulo da legenda\n    legend.text = element_text(\n      colour=\"#2e2e2e\",                    # cor da legenda \n      family = font                        # fonte\n    ),\n    plot.background = element_rect(\n      fill = \"#f7efd8\",                    # cor de fundo do grafico\n      colour = NA\n      ),\n    panel.background = element_rect(\n      fill = \"#f7efd8\",                    # cor de fundo do painel\n      colour = NA\n      )\n    \n    # since the legend often requires manual tweaking \n    # based on plot content, don't define it here\n  )\n}\nCom esse bloco de c√≥digo acima, definimos o tema personalizado para aplicarmos ao gr√°fico. O gr√°fico do Economista Visual, usa uma fonte diferente do comum, fonte que n√£o sei qual √©, e o seu t√≠tulo e sub-t√≠tulo na cor preta com ajuste a esquerda (definimos isso no tema que customizamos acima), texto nos eixos em uma cor parecido com um cinza numa escala mais clara, a mesma cor para a legenda que tamb√©m √© ajustada a esquerda, e por fim, o gr√°fico √© do tipo linha, na cor vermelha com um efeito ‚Äúglow‚Äù, ponto s√≥lido menor que o circulo, e anota√ß√£o em texto, tudo na cor vermelha.\nO eixo X √© composto pelos anos observados (datas), e a escala do eixo X inicia no ano 2000 e termina no ano 2021 com intervalos a cada 1 ano. J√° no eixo Y, temos os valores na escala de 0 a 700, com intervalo a cada 100.\nTendo essas informa√ß√µes em mente, √© hora de elaborarmos o gr√°fico. Para isso, ser√° utilizado as fun√ß√µes do pacote ggplot2 e aplicaremos o tema customizado para ajustar a est√©tica do nosso gr√°fico ao gr√°fico do Economista Visual.\nplot &lt;- CB %&gt;%              # definindo o gr√°fico\n  ggplot(aes(x=Data)) +     # adicionando apenas o eixo X\n  geom_line(                # 1¬∞ linha do eixo Y\n    aes(y = Valor),         # definindo o Valor para o eixo Y\n    size = 3,               # espessura da linha\n    colour = '#e20000',     # cor (esolhi um verlho aleatorio)\n    alpha = 0.1             # alpha mede a transpar√™ncia\n    ) +\n  geom_line(                # 2¬∞ linha do eixo Y\n    aes(y = Valor),         # definindo o Valor para o eixo Y\n    size = 2,               # espessura (menor que a 1¬∞)\n    colour = '#e20000',     # cor (a mesma cor para todas)\n    alpha = 0.2             # transpar√™ncia (maior que a 1¬∞)\n    ) +\n  geom_line(                # 3¬∞ linha do eixo Y\n    aes(y = Valor),         # definindo o Valor para o eixo Y\n    size = 1,               # espessura (menor que a 2¬∞) \n    colour = '#e20000',     # cor\n    alpha = 0.5             # transpar√™ncia (maior que a 2¬∞)\n    ) +\n  geom_line(                # 4¬∞ linha do eixo Y\n    aes(y = Valor),         # definindo o Valor para o eixo Y\n    size = 0.75,            # espessura (menor que a 3¬∞)\n    colour = '#e20000'      # cor\n    ) +                     # sem alpha, para deixar a cor s√≥lida\n  annotate(                 # 1¬∞ anota√ß√£o 1/3\n    geom=\"text\",            # tipo de anota√ß√£o: texto\n    x=as.Date(\"2000-01-01\"), # ponto que a anota√ß√£o deve aparecer\n    y=160,                  # ajustando a altura do anota√ß√£o\n    label=\"R$ 112,22\",      # Texto de refer√™ncia\n    size=4,                 # tamanho do texto\n    color = \"#e20000\",      # cor do texto (a mesma da linha)\n    family = \"Teko\"         # fonte do texto (opcional)\n    ) +\n  annotate(                 # 1¬∞ anota√ß√£o 2/3\n    geom=\"point\",           # tipo de anota√ß√£o: ponto\n    x=as.Date(\"2000-01-01\"), # ponto que a anota√ß√£o deve aparecer\n    y=112,                  # ponto cruzado entre X e Y\n    size=3,                 # tamanho do ponto\n    color = \"#e20000\"       # cor do ponto\n    ) +\n  annotate(                 # 1¬∞ anota√ß√£o 3/3\n    geom=\"point\",           # tipo de anota√ß√£o: ponto\n    x=as.Date(\"2000-01-01\"), # ponto que a anota√ß√£o de aparecer\n    y=112,                  # ponto cruzado entre X e Y\n    size=5,                 # tamanho do ponto\n    shape=21,               # formato do ponto\n    fill=\"transparent\",     # preenchimento transparente\n    color=\"#e20000\"         # cor, aplica somente na borda \n    ) +\n  annotate(                 # 2¬∞ anota√ß√£o 1/3\n    geom=\"text\",            # tipo de anota√ß√£o: texto\n    x=as.Date(\"2016-01-01\"), # ponto que a anota√ß√£o deve aparecer\n    y=500,                  # ajustando a altura da anota√ß√£o\n    label=\"R$ 448,31\",      # texto de refer√™ncia\n    size=4,                 # tamanho do texto\n    color = \"#e20000\",      # cor do texto\n    family = \"Teko\"         # fonte do texto\n    ) +\n  annotate(                 # 2¬∞ anota√ß√£o 2/3\n    geom=\"point\",           # tipo de anota√ß√£o: ponto\n    x=as.Date(\"2016-01-01\"), # ponto que a anota√ß√£o deve aparecer\n    y=448.31,               # ponto cruzado entre X e Y\n    size=3,                 # tamanho do ponto\n    color = \"#e20000\"       # cor do ponto\n    ) +\n  annotate(                 # 2¬∞ anota√ß√£o 3/3\n    geom=\"point\",           # tipo da anota√ß√£o: ponto\n    x=as.Date(\"2016-01-01\"), # ponto que a anota√ß√£o deve aparecer\n    y=448.31,               # ponto cruzado entre X e Y\n    size=5,                 # tamanho do ponto\n    shape=21,               # formato do ponto\n    fill=\"transparent\",     # preenchimento transparente\n    color=\"#e20000\"         # cor somente na borda\n    ) +\n  annotate(                 # 3¬∞ anota√ß√£o 1/3\n    geom=\"text\",            # tipo de anota√ß√£o: texto\n    x=as.Date(\"2021-01-01\"), # ponto que a anota√ß√£o deve aparecer\n    y=700,                  # ajustando a altura da anota√ß√£o\n    label=\"R$ 654,15\",      # texto de refer√™ncia\n    size=4,                 # tamanho do texto\n    color = \"#e20000\",      # cor do texto\n    family = \"Teko\"         # fonte do texto\n    ) +\n  annotate(                 # 3¬∞ anota√ß√£o 2/3\n    geom=\"point\",           # tipo de anota√ß√£o: ponto\n    x=as.Date(\"2021-01-01\"), # ponto que a anota√ß√£o deve aparecer\n    y=654.15,               # ponto cruzado entre X e Y\n    size=3,                 # tamanho do ponto\n    color = \"#e20000\"       # cor do ponto\n    ) +\n  annotate(                 # 3¬∞ anota√ß√£o 3/3\n    geom=\"point\",           # tipo de anota√ß√£o: ponto\n    x=as.Date(\"2021-01-01\"), # ponto que a anota√ß√£o deve aparecer\n    y=654.15,               # ponto cruzado entre X e Y\n    size=5,                 # tamanho do ponto\n    shape=21,               # formato do ponto\n    fill=\"transparent\",     # preenchimento transparente\n    color=\"#e20000\"         # cor somente na borda\n    ) +\n  labs(                     # r√≥tulos\n    x = NULL,               # anula r√≥tulo do eixo X\n    y = NULL,               # anula r√≥tulo do eixo Y\n    # t√≠tulo\n    title='Valor da cesta b√°sica',\n    # sub-t√≠tulo\n    subtitle = \"Com base na cidade de S√£o Paulo-SP\",\n    # legenda\n    caption='Fonte: https://www.dieese.org.br/cesta/ \\nDataViz: @hcostax'\n    ) +\n  scale_x_date(             # definindo escalas p/ eixo X\n  date_breaks = \"1 year\",   # intervalos de 1 ano\n  date_labels = \"%Y\"        # rotular somente o ano\n  ) +\n  scale_y_continuous(       # definindo escalas p/ eixo Y\n    breaks = seq(           # definindo o intervalo\n      from = 0, \n      to = 700, \n      by = 100\n      ),\n    limits=c(0, 700),       # definindo os limites\n    labels = scales::dollar_format( # adicionando nota√ß√£o monet√°ria\n      prefix=\"R$\"           # definindo como \"R$\" real brasileiro\n      )\n  ) +\n  custom_theme()            # aplicando o tema customizado\n\n# ---\n\nprint(plot)                 # gerando o gr√°fico\n\n# ---\nEsse enorme bloco de c√≥digo gera o gr√°fico final. O resultado foi bastante satisfat√≥rio (p/ mim, rsrs). Ap√≥s executar todos esses passos temos o gr√°fico:\n\nO gr√°fico n√£o ficou exatamente igual ao gr√°fico publicado no perfil do Economista Visual no instagram, mas chegamos bem pr√≥ximo. O objetivo aqui foi reproduzir a informa√ß√£o, e para deixar mais divertido, reproduzir aos moldes do original.\nVoc√™ tamb√©m pode reproduzir isso escolhendo a cidade que quiser.\nConfira o c√≥digo completo no meu Github.\n\n\n\n\n\n\nEi! üëã, voc√™ achou meu trabalho √∫til? Considere me comprar um caf√© ‚òï, clicando aqui üëáüèª\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationFor attribution, please cite this work as:\nCosta, Henrique. 2021. ‚ÄúReproduzindo Gr√°fico Do Economista Visual\n- Valor Da Cesta B√°sica (PT-BR).‚Äù February 20, 2021. https://doi.org/10.59350/y7tgs-0v282."
  },
  {
    "objectID": "blog/risk_intro/index.html",
    "href": "blog/risk_intro/index.html",
    "title": "A few words about risk management",
    "section": "",
    "text": "I started this website some time ago with the aim of writing down the expansion of my knowledge and experience in the area of financial risk management. And also other things like programming with R, econometric modeling and a little about data science and machine learning. So I‚Äôm going to use this publication as a starting point, and start discussing texts and examples applied to financial risks.\nI am not going to give a general and complete description of what financial risk is and its management processes, as this can be easily found on sites like Wikipedia.\nBut risk, in the most basic sense, is the possibility that bad things will happen. Humans have evolved to manage risks like wild animals do. However, our awareness of risk is not always adequate for the modern world.\nBehavioral science shows that we rely too much on our instincts and personal experience, as biases distort our thought processes. Furthermore, even the way we frame risk decisions irrationally influences our willingness to take risks.\nYet surprisingly sophisticated examples of risk management can be seen early in history. In ancient times, merchants and their creditors shared the risk by tying loan repayment to the safe arrival of shipments through sea loans (i.e.¬†combining loans with a type of insurance). We can see some references here and here too.\nThe insurance contract separated from the loan contract as early as the 14th century in northern Italy, creating the first autonomous financial risk transfer instrument. From the 17th century onwards, a more methodical approach to the mathematics of risk can be traced. This was followed by the development of exchange-based risk transfer, in the form of agricultural futures contracts, in the 18th and 19th centuries. Some other references here.\nThis methodical approach continued to evolve in the 20th century and beyond, with major advances in financial theory in the 1950s; an explosion in risk management markets from the 1970s onwards; and the emergence of new instruments, such as cyber risk insurance, at the beginning of the 21st century. Risk management is an ancient art, but a young science ‚Äì and an even younger profession.\nThe way we think about risk is the biggest determinant of whether we recognize risks, assess them appropriately, measure them using appropriate risk metrics and manage them.\nIn the next publications I will bring introductory content that will analyze risk definitions, the classic risk management process, the main types of risk and the tools used to track risks and make decisions.\nMost risk management disasters are caused by the failure to recognize and/or adequately address one or more of these fundamental elements, not by the failure of some sophisticated risk management technique. Century-old financial institutions have gone bankrupt because their risk management procedures ignored a certain type of risk, misunderstood the links between risks, or did not follow the classic steps of the risk management process. As we recently observed the failure of some financial institutions, both in Brazil and around the world.\n\n\n\n\n\n\nHey! üëã, did you find my work useful? Consider buying me a coffee ‚òï, by clicking here üëáüèª\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationFor attribution, please cite this work as:\nCosta, Henrique. 2023. ‚ÄúA Few Words about Risk Management.‚Äù\nAugust 27, 2023. https://doi.org/10.59350/51paw-dza08."
  },
  {
    "objectID": "blog/modelling_foundations/index.html",
    "href": "blog/modelling_foundations/index.html",
    "title": "Trivia Series: Modelling foundations",
    "section": "",
    "text": "Modeling Principles\nAlmost all, if not all, applied science is based on the idea of a model. There are problems in the real world, and we would like to create a theory to solve such problems.\nOne of the pillars or the main foundation of quantitative modeling is its effectiveness. But how can a model be effective? By demonstrating that it is simple, reproducible, and applicable in other areas of knowledge.\nA second pillar of modeling is the ability to make decisions and choices. And such choices are, and should be, made at any and all times in the process of specifying a model.\nAnother pillar is testing. A model cannot exist without its due testing process. These tests are experiments, where choices must be made and tested to ensure the effectiveness of the model.\nA model has a simple objective: to be a simplistic representation of the real world. Based on the central limit theorem, sampling, and randomness, a model is nothing more than a specific/local/unitary idea that represents the general/total.\nThe sample will never be equal to the population, and a unit will never be the total, but a model finds a pattern that it will assume as central and how far the units are from this pattern, which we formally call ‚Äúeffect‚Äù and ‚Äúresidue‚Äù.\nThe ‚Äúeffect‚Äù is what we seek, in a simplified way, to solve the real problem, and the ‚Äúresidue‚Äù is what corroborates our effect, the residue is the acid test of the model and will demonstrate how too simple the model is, or how too complex the model is.\nIt is necessary to move between the simple and the complex to centralize the effect. If the effect is central, and the residue is coherent and parsimonious, then the model has fulfilled its main objective: in a simple way, to represent the complex reality.\nAnd here is how this process works:\n\n\nThe problems observed in the real world are very complex, and to solve them we need to look for the exact solution.\nBut if the problem is complex, the solution must be just as complex, and that is why it is necessary to simplify. We can solve some problems with approximate solutions, and this is feasible. However, an approximate solution requires an approximate problem.\nIn this sense, we observe the real problem and conditionally create a simplified version that we call an approximate problem, since it is in fact close to the real one, only some simplifications have occurred to allow us to study, estimate, test and theorize.\nFrom the approximated (simplified) problem it is possible to obtain an exact solution. Therefore, it is feasible that the exact solution of the approximated problem is the approximate solution of the real problem.\nAnd so the fundamental process of modeling occurs:\n\nThis very simplistic process allows us to see that the model is nothing more than a simplification of the real problem, as I mentioned before. Using the model, it is possible to analyze the results found without needing to have the solution. The analysis of the model serves to understand the nuances of the results in order to draw conclusions about the simplification that was created. Sometimes we will not have the solution we are looking for, but we can have conclusions, even if simplified, about the real problem.\nThe analysis and conclusions of the model allow us to generate inference, which is the deduction made based on the reasoning generated by the analysis, and from the inference we can explain, even if in a simplified way, the real problem and make estimates and predictions.\nThe final step is the decision or choice that we make based on the understanding and estimates. With this, we monitor and verify whether our model is sufficient to reach an approximate solution to the real problem. Therefore, we verify whether our choices for applying the model results are meeting the expectations based on the estimates of our model.\nIf our model is not sufficient, that is, if the estimates are not observed in the real world, we need to recalibrate this model or even specify a new one. This means that we failed in one of the steps outlined in the flows above.\nWe usually make a lot of mistakes when simplifying the problem, in the initial specification of the model. It seems trivial, and it is, but this step requires a lot of creativity, because modeling is an art. If we simplify the model too much, it loses its sensitivity to reality, and if we simplify it too little, the model can be so complex that it cannot be distinguished from the real problem, and this causes failure, due to the difficulty in obtaining an exact solution.\nThe second point where we fail the most is in decision-making and verification, because this process can be expensive. Monitoring is a cost that may never be proposed or included in a project (specifying a model). The choice of how to apply it can also be made without due proportions. In general, the decision-making process can be poorly done, generating the inconsistency that leads the model to failure.\nGenerally, the person who makes the decisions is not always the same person who collected the information, nor may it be the person who specified and executed the model, much less the person who performed the prior analysis and reached the conclusions and inferences.\nThe modeling process is a process that has an indefinite temporality, but it has steps that need to be respected, as if it were a natural rule. So far, these rules are valid and work very well. For example, we have many powerful models that are sometimes defined theoretically and, when applied, perform their role accurately.\nThere is no defined time, but it can have a cost. The longer it takes, the higher the cost. This is only valid if, and only if, time has a monetary value (such as working hours, the researcher‚Äôs salary or the project budget).\nSo, even if it is trivial, do not ignore this knowledge. A model is not specified in minutes, and executing a famous or currently fashionable algorithm is not enough. It is necessary to abstract in order to think about the complexity of the real problem, as well as to think about the conditioning paths of simplification.\nA very interesting topic in simplifying a real problem and finding an approximate solution is the idea of ceteris paribus used in economics, where the general idea is to apply, in a subjective and abstract way, an implicit equilibrium condition in which the relationship between phenomena is balanced and therefore any change in the other phenomena not used in the model has no effect, isolating only the phenomenon chosen to estimate the effect of one phenomenon on the other in order to draw conclusions, make inferences and make decisions.\nFrom now on, you already know how to start your modeling process, which is not just visiting your data lake, selecting a multitude of variables and running a convolutional neural network calculation and hoping that your model is as accurate as your accuracy test tells you it is. Sometimes, a linear regression with 3 variables is enough, and this would be following the first step of simplification.\n\n\n\n\n\n\nHey! üëã, did you find my work useful? Consider buying me a coffee ‚òï, by clicking here üëáüèª\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationFor attribution, please cite this work as:\nCosta, Henrique. 2024. ‚ÄúTrivia Series: Modelling\nFoundations.‚Äù August 25, 2024. https://doi.org/10.59350/rzbj1-p3865."
  },
  {
    "objectID": "blog/maths_economics/index.html",
    "href": "blog/maths_economics/index.html",
    "title": "The mathematization of Economics",
    "section": "",
    "text": "In today‚Äôs post, I want to start a discussion about the use of mathematics and statistics in economics, and with this shocking sentence:\n\nWhen a person studies economics for the first time, it is likely that they will not encounter ‚Äúcrazy‚Äù equations that go beyond basic mathematics. There is a lot of content to learn the various conceptual definitions such as price, supply, demand, costs, profit, etc., in addition to the various market structures.\nAs you delve deeper into the subject, you realize there is more to it than just simplistic theories and newspaper chatter. So, what would be the best way to explain the concepts of price, quantity of product sold and production cost without referring to a single example without using mathematics?\nThis paper:\n\nKATZNER, Donald W. Why mathematics in economics?. Journal of Post Keynesian Economics, v. 25, n.¬†4, p.¬†561-574, 2003.\n\nmakes a very convincing defense of the use of mathematics in economic science.\nAlthough economics is technically recognized as a social science, students who embark on a ‚Äúnebulous‚Äù adventure (undergraduate course) in this field receive (or should receive) a solid foundation in mathematics. Because determining how scarce resources can be reallocated requires a minimum understanding of mathematics, to calculate what the distribution cost will be and evaluate (quantitatively) among other measures. Thus, the field of economics is full of mathematical equations and applications.\nWhat is learned in teaching economics are mainly linear algebra, calculus and statistics. These are the basis for achieving the infamous econometrics. In algebra it is taught about total cost and total revenue. In calculus the objective is to find the derivatives of indifference and utility curves, profit maximization curves, cost minimization and growth models.\nIn statistics, economists learn about predictive models and how to determine how likely a certain event is to occur. Econometrics, for some peculiar reasons, is called ‚Äúeconomentira‚Äù for the use of models in which the final objective is to estimate and predict a certain variable.\n\nAs you progress through related topics, you will find examples such as market demand curves (sum of several individual demand curves) or changes in the supply and price of a commodity or calculating the price elasticity of a commodity. consumption, each concept is validated using mathematics. Definitely, a mathematical and statistical approach is needed to have clarity when we arrive at the long-awaited ‚Äúsolution‚Äù to the problems proposed to professionals in this area.\n\n\n\nA typical review for the basic econometrics final exam\n\n\nIt has been noted that in the 19th century mathematics was considered as a means of achieving truth; (rational) logic made it imperative to use mathematics to prove any theorems. Many problems posed in economics are therefore motivated to be solved by mathematics. Have these problems really been solved?\nAnalyzes and studies carried out in the field of applied economics help to explain the interdependent relationship between different variables. An example is trying to explain what causes an increase in the price of a product, such as the price of beef, or an increase in the unemployment rate, or even a drop in inflation and a reduction in the basic interest rate. Mathematical functions are used as a logical tool through models from which these phenomena of everyday life can become more understandable.\nIn fact, there is an exhaustive discussion about the importance of relevant applied work and the uses of these metrics in economic science. It is interesting to know that several economists have been awarded the Nobel Prize for applying mathematics/statistics to economics, including the first awarded in 1969 to Ragnar Frisch and JanTinbergen. The most interesting thing is that Leonid Kantorovich he won a Nobel Prize in 1975 in economics for his contribution to the theory of optimal resource utilization and he was a mathematician!\nMany students who are looking to pursue a career in economics are advised to take a Mathematics course, as applied studies are covered in mathematics. The use of mathematical models and applications in this area has been notable in the last two decades.\nEconomics ‚Äî the science that stopped being dark(In Brazilian Portuguese) , with advances in Alfred Marshall, with the well-known marginalist revolt that embraced the use of mathematics as an integral part of the economy, now more intensive than ever. Mathematics plays the main role in many sciences like physics, chemistry, etc. And it‚Äôs really the backbone of the modern economy.\nMathematics in economics is an important tool in decision making. Economists are hired by companies or governments to investigate and estimate the risk or likely outcomes of an event. Economists who work for companies in the financial market make mathematical calculations (models) to assess whether the risk of investing in a certain asset outweighs its potential benefits.\nEconomists use their mathematical ‚Äúskills‚Äù to find ways to reallocate money, even in counterintuitive ways. Using a profit-maximization chart, economists can advise a venue to sell only 75% of available tickets, rather than 100%, a strategy to maximize its profit. If the company lowers ticket prices to attract new concert-goers and fill the stadium to capacity, it could make less money than selling just 75% of the tickets at a much higher price.\nEconomists also use mathematics to determine the long-term success of a business, even when some factors are unpredictable. For example, an economist working for an airline uses forecasting based on econometric models to determine the price of fuel for two months ahead. The company uses this data to block fuel prices or to protect the fuel, the famous ‚Äúhedge‚Äù. Bijan Vasigh, author of the book ‚ÄúIntroduction to Air Transport Economics‚Äù, explains that the Southwest Airlines gained a financial advantage over other operators due to its fuel hedging strategy.\nHowever, not everything is rosy, much less a bed of roses, there are limitations to what can be done using econometric models in the economy. Economists perform mathematical calculations with imperfect information. Their economic models are useless in times of natural disasters, union strikes or any other catastrophic event. Furthermore, mathematics rarely helps economists predict irrational human behavior. A fundamental assumption of economics is that humans act rationally. However, humans often make irrational decisions based on choices and preferences, or even pure fear or love. These two factors cannot be accounted for in an economic model.\nBut there are already some techniques, including in the field of Machine Learning that try to solve such problems. But that is the subject of another post that I will make soon.\nBut the potential of these methods is recognized and therefore extremely useful, and economists are reviewing the way calculations are carried out to account for intangible effects, such as pollution. Mathematical models are necessarily based on simplifying assumptions, so they are not likely to be perfectly realistic. Mathematical models also lack the nuances that can be found in narrative models. The point is that mathematics is a tool, but it is not the only tool or even the best tool that economists should use.\nSo, what is your opinion regarding the use of mathematics and statistics in economic science? Is it possible to think about economics without at least mathematizing it?\nAdapted from original.\n\n\n\n\n\n\nHey! üëã, did you find my work useful? Consider buying me a coffee ‚òï, by clicking here üëáüèª\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationFor attribution, please cite this work as:\nCosta, Henrique. 2018. ‚ÄúThe Mathematization of Economics.‚Äù\nSeptember 21, 2018. https://doi.org/10.59350/tvp77-8gp37."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Henrique Costa",
    "section": "",
    "text": "Hi there! I‚Äôm a quant risk management professional + Master in Applied Economics, and passionate about empirical validation analysis.\nI have ‚Äúrevealed preference‚Äù for matcha üçµ and coffee ‚òï, pizza üçï, microeconomics, econometrics, microeconometrics and R.\nA concept that I always apply is: The whole intention of empirical economics is to force theory down to Earth. - George Akerlof.\nMy motto is: Be better than average.\nAt my house, my wife and I play with our children Arya, Frodo and Lunna (Three beautiful kittens). We love and enjoy brunch on Sundays and are always willing to try great food."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact ",
    "section": "",
    "text": "Contact \nYou can use this form to contact me to say hello. I‚Äôm always willing to read and listen to anyone who wants to talk about economics, data science, risk management, or statistics.\nAny time spent talking to people on the Internet about these topics is time saved. I‚Äôm the annoying friend who loves talking about curiosities and refusing to talk about normal things (I don‚Äôt know what normal people talk about).\nI also love knowing if my materials were helpful to you and how they could be improved ‚Äì especially if they could be more accessible.\nHowever, for queries related to consultancy, collaborations or speaking engagements, please visit the consultancy page.\n\n\n\n\n\n\n\n\nFull Name \nEmail Address \nMessage\n\n\nSend message"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Project Portfolio",
    "section": "",
    "text": "[PT-BR] Casting lots and placing bets: decision making under uncertainty and risk management - Talking/Presentation for the master‚Äôs program in economics (UFMT). April 16, 2024.\nFoundations of Risk Management (Working in Progress)\nCycle Theory and Risk Management (Working in Progress)\n{crmvR} - A Practical R package for IFRS 9 and CECL Credit Risk Modelling and Validation. (Working in Progress)"
  },
  {
    "objectID": "projects/index.html#quant-finance-and-risk-management",
    "href": "projects/index.html#quant-finance-and-risk-management",
    "title": "Project Portfolio",
    "section": "",
    "text": "[PT-BR] Casting lots and placing bets: decision making under uncertainty and risk management - Talking/Presentation for the master‚Äôs program in economics (UFMT). April 16, 2024.\nFoundations of Risk Management (Working in Progress)\nCycle Theory and Risk Management (Working in Progress)\n{crmvR} - A Practical R package for IFRS 9 and CECL Credit Risk Modelling and Validation. (Working in Progress)"
  },
  {
    "objectID": "projects/index.html#papers-research-working-in-progress",
    "href": "projects/index.html#papers-research-working-in-progress",
    "title": "Project Portfolio",
    "section": "Papers & Research (Working in Progress) ",
    "text": "Papers & Research (Working in Progress) \n\n[PT-BR] COSTA, Henrique & SOARES, Jadson. ‚ÄúAvalia√ß√£o espacial da inadimpl√™ncia (Non-Performing Loans) dos estados brasileiros baseado nos indicadores locais de associa√ß√µes espaciais (LISA)‚Äù. (Working in Progress)\nCOSTA, Henrique. ‚ÄúVintage Analysis: a tool for risk management‚Äù. (Working in Progress)\nCOSTA, Henrique. ‚ÄúX-ray of credit default in Brazil‚Äù. (Working in Progress)"
  },
  {
    "objectID": "blog/master_bank_study/index.html#infer√™ncia-causal-de-resultados-potenciais-√†-arquitetura-gr√°fica",
    "href": "blog/master_bank_study/index.html#infer√™ncia-causal-de-resultados-potenciais-√†-arquitetura-gr√°fica",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "Infer√™ncia Causal: De Resultados Potenciais √† Arquitetura Gr√°fica",
    "text": "Infer√™ncia Causal: De Resultados Potenciais √† Arquitetura Gr√°fica\nA infer√™ncia causal moderna √© o ponto de partida deste estudo. Fundamentada nas obras de Donald Rubin (1974) e Judea Pearl (2009), ela fornece instrumentos para formalizar a pergunta central que guia este trabalho: ‚ÄúO que teria acontecido se‚Ä¶‚Äù.\nRubin prop√µe que causalidade √© definida como a diferen√ßa entre dois estados potenciais de um mesmo objeto: o mundo em que recebe o tratamento \\((Y(1)Y(1))\\) e o mundo em que n√£o recebe \\((Y(0)Y(0))\\). O problema fundamental da infer√™ncia causal ‚Äî a impossibilidade de observar simultaneamente ambos os estados ‚Äî √© resolvido via simula√ß√µes, compara√ß√µes e repondera√ß√µes.\nPearl expande esse arcabou√ßo ao construir o conceito de Diagramas Ac√≠clicos Direcionados (DAGs). Um DAG √© uma representa√ß√£o gr√°fica onde vari√°veis s√£o conectadas por setas orientadas, explicitando n√£o apenas correla√ß√µes, mas rela√ß√µes causais dirigidas.\nA opera√ß√£o \\(do(X)\\) introduz a capacidade de modelar interven√ß√µes: simular artificialmente o que aconteceria ao alterar \\(X\\) enquanto isolamos as demais rela√ß√µes.\nEste estudo adota a perspectiva de Pearl: eventos no colapso do Banco Master s√£o organizados como n√≥s em um DAG, e interven√ß√µes hipot√©ticas (ex: auditoria externa, aus√™ncia de precat√≥rios) s√£o modeladas para observar trajet√≥rias alternativas.\nContudo, como veremos a seguir, a causalidade aqui n√£o √© tratada como um dado absoluto ‚Äî mas como uma constru√ß√£o epistemol√≥gica, sujeita a limites e interpreta√ß√µes."
  },
  {
    "objectID": "blog/master_bank_study/index.html#causalidade-como-constru√ß√£o-a-cr√≠tica-epistemol√≥gica",
    "href": "blog/master_bank_study/index.html#causalidade-como-constru√ß√£o-a-cr√≠tica-epistemol√≥gica",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "Causalidade como Constru√ß√£o: a cr√≠tica epistemol√≥gica",
    "text": "Causalidade como Constru√ß√£o: a cr√≠tica epistemol√≥gica\nPara al√©m do rigor matem√°tico de Rubin e Pearl, este estudo reconhece a dimens√£o cr√≠tica da causalidade apontada por autores como Nancy Cartwright (2007) e Ian Hacking (1983).\nCartwright sustenta que modelos causais n√£o descrevem leis naturais universais, mas estruturas locais e espec√≠ficas, dependentes do contexto e das escolhas do modelador. Ela argumenta que ‚Äúleis n√£o voam sozinhas‚Äù ‚Äî ou seja, o sucesso de uma previs√£o causal depende da exist√™ncia de mecanismos que sustentem aquela rela√ß√£o no ambiente modelado.\nHacking refor√ßa essa posi√ß√£o ao afirmar que a causalidade √© uma forma de interven√ß√£o, n√£o de observa√ß√£o pura. N√£o ‚Äúdescobrimos‚Äù causas como arque√≥logos ‚Äî intervimos, testamos, reconfiguramos o mundo para revelar padr√µes.\nNo contexto do Banco Master, essa vis√£o cr√≠tica implica que:\n\nO DAG proposto n√£o √© uma fotografia fiel da realidade,\nEle √© uma estrutura plaus√≠vel, constru√≠da para organizar rela√ß√µes prov√°veis de causalidade a partir dos dados dispon√≠veis,\nAs infer√™ncias realizadas s√£o condicionadas √† estrutura modelada, e n√£o √† natureza do sistema em si.\n\nEssa postura epistemol√≥gica √© essencial para interpretar corretamente os resultados simulados: eles n√£o s√£o verdades absolutas, mas narrativas probabil√≠sticas ancoradas em escolhas te√≥ricas conscientes."
  },
  {
    "objectID": "blog/master_bank_study/index.html#mundos-poss√≠veis-a-estrutura-contrafactual",
    "href": "blog/master_bank_study/index.html#mundos-poss√≠veis-a-estrutura-contrafactual",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "Mundos Poss√≠veis: A Estrutura Contrafactual",
    "text": "Mundos Poss√≠veis: A Estrutura Contrafactual\nInspirado na interpreta√ß√£o de muitos mundos de Hugh Everett III (1957), este estudo adota a ideia de que cada combina√ß√£o alternativa de eventos gera um universo poss√≠vel distinto. N√£o h√° apenas um futuro ‚Äî h√° uma √°rvore imensa de trajet√≥rias potenciais, divergindo a cada decis√£o ou circunst√¢ncia.\nEssa perspectiva √© operacionalizada atrav√©s de:\n\nSimula√ß√µes bayesianas de m√∫ltiplos mundos,\nSorteio aleat√≥rio de pesos causais para cada universo,\nExplora√ß√£o sistem√°tica das condi√ß√µes sob as quais o colapso ocorre ou √© evitado.\n\nA no√ß√£o de mundos poss√≠veis permite ultrapassar a limita√ß√£o dos modelos deterministas tradicionais: n√£o perguntamos apenas ‚Äúo que aconteceu?‚Äù, mas ‚Äúo que poderia ter acontecido sob outras combina√ß√µes causais?‚Äù.\nNo Banco Master, isso significa mapear n√£o apenas a trajet√≥ria real que levou √† liquida√ß√£o, mas tamb√©m:\n\nUniversos em que auditorias externas foram implementadas a tempo;\nUniversos em que a governan√ßa foi refor√ßada;\nUniversos em que aquisi√ß√µes de precat√≥rios n√£o ocorreram.\n\nCada simula√ß√£o representa um observ√°vel alternativo ‚Äî uma realiza√ß√£o de uma trajet√≥ria poss√≠vel."
  },
  {
    "objectID": "blog/master_bank_study/index.html#colapsabilidade-fragilidade-estrutural-e-instabilidade-end√≥gena",
    "href": "blog/master_bank_study/index.html#colapsabilidade-fragilidade-estrutural-e-instabilidade-end√≥gena",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "Colapsabilidade, Fragilidade Estrutural e Instabilidade End√≥gena",
    "text": "Colapsabilidade, Fragilidade Estrutural e Instabilidade End√≥gena\nA colapsabilidade causal ‚Äî conceito central deste estudo ‚Äî refere-se √† frequ√™ncia com que o sistema modelado, a partir de suas condi√ß√µes estruturais, converge para o colapso.\nEssa m√©trica √© inspirada na tradi√ß√£o de:\n\nHyman Minsky (2008): para quem a estabilidade econ√¥mica gera complac√™ncia, incentivando a alavancagem excessiva at√© o ponto de ruptura.\nNassim Taleb (2012): que introduz a no√ß√£o de antifragilidade como capacidade de aprender e crescer com a volatilidade ‚Äî e contrap√µe sistemas fr√°geis, que colapsam facilmente.\n\nO Banco Master, segundo o modelo proposto, n√£o colapsou porque enfrentou um choque externo at√≠pico. Ele colapsou porque sua estrutura era, em si, propensa ao colapso: governan√ßa centralizada, exposi√ß√£o elevada a riscos judiciais, funding inst√°vel e vulnerabilidade reputacional.\nA simula√ß√£o de mil mundos permite quantificar essa propens√£o: mesmo variando condi√ß√µes iniciais e pesos causais, a grande maioria das trajet√≥rias (85,7%) resulta em fal√™ncia.\nA fragilidade, aqui, √© uma propriedade emergente da rede de rela√ß√µes causais ‚Äî n√£o o produto de um √∫nico evento."
  },
  {
    "objectID": "blog/master_bank_study/index.html#cr√≠tica-√†-modelagem-tradicional-risco-n√£o-√©-n√∫mero-√©-estrutura",
    "href": "blog/master_bank_study/index.html#cr√≠tica-√†-modelagem-tradicional-risco-n√£o-√©-n√∫mero-√©-estrutura",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "Cr√≠tica √† Modelagem Tradicional: Risco n√£o √© N√∫mero, √© Estrutura",
    "text": "Cr√≠tica √† Modelagem Tradicional: Risco n√£o √© N√∫mero, √© Estrutura\nA modelagem tradicional de riscos financeiros, baseada em frameworks como o Value at Risk (VaR), stress tests param√©tricos e modelos de regress√£o linear, repousa sobre uma s√©rie de suposi√ß√µes que, embora convenientes matematicamente, tornam-se perigosas epistemologicamente.\nEsses modelos pressup√µem:\n\nEstabilidade estrutural: o sistema financeiro √© tratado como essencialmente est√°tico, reagindo apenas a choques externos.\nNormalidade estat√≠stica: distribui√ß√µes de perdas seguem padr√µes gaussianos, em que eventos extremos s√£o rar√≠ssimos.\nLinearidade causal: choques afetam o sistema de maneira proporcional e previs√≠vel.\nSeparabilidade dos riscos: riscos s√£o somados ou compostos de maneira aditiva, sem efeitos de intera√ß√£o complexa.\n\nNo entanto, como argumentam Minsky (2008) e Taleb (2012), essas premissas n√£o descrevem o comportamento real de sistemas financeiros.\nO risco sist√™mico emerge da pr√≥pria estrutura relacional dos agentes, e n√£o de choques isolados. O colapso n√£o √© um ponto fora da curva ‚Äî √© o resultado end√≥geno da arquitetura de fragilidade acumulada.\nAl√©m disso, como mostram Pearl (2009) e Cartwright (2007), causalidade n√£o √© agrega√ß√£o de vari√°veis:\n\nCausalidade exige entender como vari√°veis se conectam,\nquais interven√ß√µes alteram desfechos,\ne quais trajet√≥rias s√£o poss√≠veis sob combina√ß√µes diferentes de eventos.\n\nOs modelos tradicionais, ao ignorarem a arquitetura causal, tornam-se cegos para a progress√£o do risco estrutural.\nEles podem medir a volatilidade de uma vari√°vel financeira isolada ‚Äî mas n√£o enxergam a forma√ß√£o lenta e cont√≠nua de padr√µes de colapso que atravessam m√∫ltiplos dom√≠nios.\n\nO novo paradigma adotado neste estudo\nEm contraposi√ß√£o, este trabalho prop√µe um deslocamento de paradigma:\n\nDo ponto √≥timo para o espa√ßo de possibilidades: N√£o buscamos prever um valor esperado de perda, mas explorar os m√∫ltiplos caminhos causais que podem emergir.\nDa normalidade para a instabilidade estrutural: Assumimos que sistemas financeiros s√£o intrinsecamente propensos √† instabilidade, como defende Minsky.\nDa previs√£o estat√≠stica para a modelagem causal: Substitu√≠mos proje√ß√µes pontuais por simula√ß√µes de m√∫ltiplos mundos, testando a robustez estrutural frente a varia√ß√µes param√©tricas e relacionais.\nDo risco como n√∫mero para o risco como arquitetura: O risco n√£o √© uma vari√°vel ‚Äî √© a configura√ß√£o causal do sistema como um todo.\n\n\n\nAplica√ß√£o no caso Banco Master\nAo analisar o Banco Master sob esta lente, compreendemos que:\n\nO colapso n√£o foi resultado de um ‚Äúevento extremo‚Äù imprevis√≠vel.\nEle foi o desfecho natural da arquitetura causal constru√≠da ao longo do tempo: Governan√ßa fr√°gil ‚Üí Alavancagem judicial ‚Üí Exposi√ß√£o reputacional ‚Üí Funding agressivo ‚Üí Rea√ß√£o regulat√≥ria tardia ‚Üí Colapso.\n\nEssas rela√ß√µes n√£o s√£o lineares nem aditivas ‚Äî elas s√£o interativas e acumulativas.\nO modelo causal aqui proposto, ao explorar 1.000 mundos poss√≠veis, reconhece o risco como uma propriedade emergente da estrutura relacional do sistema ‚Äî e n√£o como uma anomalia estat√≠stica."
  },
  {
    "objectID": "blog/master_bank_study/index.html#estrutura-geral-do-modelo",
    "href": "blog/master_bank_study/index.html#estrutura-geral-do-modelo",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "Estrutura Geral do Modelo",
    "text": "Estrutura Geral do Modelo\nO estudo desenvolve uma simula√ß√£o anal√≠tica baseada em infer√™ncia causal gr√°fica e probabilidade bayesiana para modelar o colapso ou sobreviv√™ncia do Banco Master.\nO procedimento baseia-se em:\n\nConstru√ß√£o de um Diagrama Ac√≠clico Direcionado (DAG) representando rela√ß√µes causais entre vari√°veis cr√≠ticas;\nAtribui√ß√£o de pesos causais \\(\\theta_{ij}\\) baseados em distribui√ß√µes probabil√≠sticas (priors) distintas;\nSimula√ß√£o de 1.000 mundos poss√≠veis, cada qual uma realiza√ß√£o independente dos pesos e eventos;\nAplica√ß√£o de Pondera√ß√£o por Escore de Propens√£o (IPW) para estimar efeitos marginais ajustados;\nC√°lculo de m√©tricas estruturais de colapsabilidade e impacto marginal.\n\nA arquitetura metodol√≥gica visa n√£o apenas reproduzir o evento observado (colapso), mas explorar trajet√≥rias contrafactuais plaus√≠veis e medir a fragilidade estrutural do sistema."
  },
  {
    "objectID": "blog/master_bank_study/index.html#constru√ß√£o-do-dag-representa√ß√£o-causal-estruturada",
    "href": "blog/master_bank_study/index.html#constru√ß√£o-do-dag-representa√ß√£o-causal-estruturada",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "Constru√ß√£o do DAG: Representa√ß√£o Causal Estruturada",
    "text": "Constru√ß√£o do DAG: Representa√ß√£o Causal Estruturada\nA primeira etapa consistiu na modelagem expl√≠cita das rela√ß√µes causais mais relevantes associadas √† trajet√≥ria do Banco Master.\nO DAG proposto inclui n√≥s representando:\n\nGovernan√ßa Fr√°gil\nEstrutura Societ√°ria Concentrada\nSinais de Fraude Cont√°bil\nAquisi√ß√£o de Precat√≥rios\nFunding Agressivo (CDBs)\nFIDC N√£o Transparente\nWill Bank e Voiter Com Problemas\nAmbiente Pol√≠tico-Institucional Permissivo\nInterven√ß√£o Regulat√≥ria Tardia (Bacen atuou)\nColapso (Desfecho Final)\n\nCada vari√°vel \\(X_j\\) √© modelada como uma fun√ß√£o de seus pais causais \\(\\text{pa}(j)\\):\n\\[\nX_j = f\\left( \\sum_{i \\in \\text{pa}(j)} \\theta_{ij} X_i + \\varepsilon_j \\right)\n\\]\nOnde: - \\(\\theta_{ij}\\) s√£o os pesos causais atribu√≠dos √†s conex√µes entre vari√°veis; - \\(\\varepsilon_j \\sim \\mathcal{N}(0, \\sigma^2)\\) √© o ru√≠do aleat√≥rio (varia√ß√µes n√£o modeladas); - \\(f(\\cdot)\\) √© uma fun√ß√£o de liga√ß√£o (ex: log√≠stica ou degrau), conforme a natureza de \\(X_j\\).\nAs dire√ß√µes seguem a l√≥gica te√≥rica, com: Bloqueio de backdoors: Controle de vari√°veis de confus√£o. Tratamento de colisores: ‚ÄúColapso‚Äù age como um collider no grafo.\n\\[ f(x) = \\begin{cases}\n\\frac{1}{1 + e^{-x}} & \\text{(Log√≠stica)} \\\\\n\\mathbb{I}(x &gt; 0) & \\text{(Degrau)}\n\\end{cases} \\]"
  },
  {
    "objectID": "blog/master_bank_study/index.html#simula√ß√£o-bayesiana-de-m√∫ltiplos-mundos",
    "href": "blog/master_bank_study/index.html#simula√ß√£o-bayesiana-de-m√∫ltiplos-mundos",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "Simula√ß√£o Bayesiana de M√∫ltiplos Mundos",
    "text": "Simula√ß√£o Bayesiana de M√∫ltiplos Mundos\nA segunda etapa envolveu a gera√ß√£o de 1.000 mundos simulados.\nEm cada mundo:\nEm cada mundo simulado:\n\nAtribui√ß√£o de pesos causais:\nA cada arco causal \\(i \\to j\\) √© associado um peso \\(\\theta_{ij}^{(s)}\\) amostrado de uma distribui√ß√£o pr√©-definida.\nDistribui√ß√µes de priors testadas:\n\nBeta-like (2,2):\nPeso m√©dio centrado (cen√°rio de conhecimento emp√≠rico);\nGaussiana truncada (\\(\\mu = 0.25\\), \\(\\sigma = 0.15\\)):\nVi√©s negativo, simulando omiss√£o ou debilidade estrutural;\nUniforme truncada (0.1 a 0.9):\nIncerteza radical para teste de robustez.\n\n\nA cada itera√ß√£o:\n\nOs pesos \\(\\theta_{ij}^{(s)}\\) s√£o sorteados conforme o prior escolhido;\nAs vari√°veis \\(X_j\\) s√£o ativadas seguindo a topologia causal;\nO desfecho de colapso \\(Y\\) √© calculado pela fun√ß√£o log√≠stica:\n\n\\[\n\\mathbb{P}(Y = 1) = \\sigma\\left( \\sum_j \\gamma_j X_j \\right)\n\\]\nOnde \\(\\gamma_j\\) s√£o pesos finais espec√≠ficos associados a vari√°veis terminalmente cr√≠ticas.\nO operador \\(\\sigma(z)\\) √© a fun√ß√£o log√≠stica:\n\\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\]\nEssa estrutura permite que o colapso n√£o seja determinado por uma √∫nica vari√°vel, mas pela agrega√ß√£o ponderada de riscos ao longo da rede causal."
  },
  {
    "objectID": "blog/master_bank_study/index.html#pondera√ß√£o-por-escores-de-propens√£o-ipw",
    "href": "blog/master_bank_study/index.html#pondera√ß√£o-por-escores-de-propens√£o-ipw",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "Pondera√ß√£o por Escores de Propens√£o (IPW)",
    "text": "Pondera√ß√£o por Escores de Propens√£o (IPW)\nPara avaliar o impacto marginal de vari√°veis bin√°rias cr√≠ticas ‚Äî como presen√ßa de auditoria externa ou aus√™ncia de precat√≥rios ‚Äî foi utilizada a t√©cnica de Inverse Probability Weighting (IPW).\nProcedimento:\n\nModelou-se a probabilidade de tratamento \\(e(\\mathbf{X}) = \\mathbb{P}(D = 1|\\mathbf{X})\\) via regress√£o log√≠stica, onde \\(D\\) representa o tratamento/interven√ß√£o (ex: auditoria).\nCada mundo simulado recebeu um peso:\n\n\\[\nw_i = \\frac{D_i}{e(\\mathbf{X}_i)} + \\frac{1 - D_i}{1 - e(\\mathbf{X}_i)}\n\\]\n\nOs efeitos marginais (ATT - Average Treatment Effect on the Treated) foram estimados reponderando as simula√ß√µes pelo IPW, isolando assim o impacto causal das interven√ß√µes.\n\n\nNota t√©cnica:\n\nN√£o utilizamos matching de escores de propens√£o, para evitar perda de simula√ß√µes e maximizar o aproveitamento dos mundos simulados.\nIPW foi preferido pois permite ajustar todo o espa√ßo de possibilidades simultaneamente."
  },
  {
    "objectID": "blog/master_bank_study/index.html#defini√ß√£o-das-m√©tricas-de-avalia√ß√£o",
    "href": "blog/master_bank_study/index.html#defini√ß√£o-das-m√©tricas-de-avalia√ß√£o",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "Defini√ß√£o das M√©tricas de Avalia√ß√£o",
    "text": "Defini√ß√£o das M√©tricas de Avalia√ß√£o\nA metodologia extrai tr√™s m√©tricas principais dos mundos simulados:\n\nColapsabilidade Causal:\n\n\\[\n\\text{Colapsabilidade} = \\frac{1}{S} \\sum_{s=1}^{S} 1[Y^{(s)} = 1]\n\\]\nPercentual de mundos onde ocorre o colapso, refletindo a propens√£o estrutural √† fal√™ncia.\n\nImpacto Marginal (ATT) das Interven√ß√µes:\n\n\\[\n\\hat{T}_{ATT} = \\frac{1}{n_t} \\sum_{i \\in D=1} w_i \\left( Y_i - \\hat{Y}^{(0)}_i \\right)\n\\]\nImpacto de vari√°veis bin√°rias na probabilidade de colapso.\n\nResili√™ncia Estrutural:\n\nFrequ√™ncia de mundos onde, mesmo sob risco ativado, o banco sobreviveu ‚Äî permitindo identificar trajet√≥rias de sobreviv√™ncia estrutural."
  },
  {
    "objectID": "blog/master_bank_study/index.html#arquitetura-detalhada-da-dag-e-das-rela√ß√µes-causais",
    "href": "blog/master_bank_study/index.html#arquitetura-detalhada-da-dag-e-das-rela√ß√µes-causais",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "Arquitetura Detalhada da DAG e das Rela√ß√µes Causais",
    "text": "Arquitetura Detalhada da DAG e das Rela√ß√µes Causais\nO Diagrama Ac√≠clico Direcionado (DAG) foi constru√≠do para representar, de maneira expl√≠cita e orientada, as principais rela√ß√µes causais que determinam a trajet√≥ria do Banco Master.\nO DAG foi organizado em quatro blocos causais:\n\nGovernan√ßa e Estrutura Institucional\n\nGovernan√ßa Fr√°gil\nEstrutura Societ√°ria Concentrada\nSinais de Fraude Cont√°bil\n\nDecis√µes Estrat√©gicas de Neg√≥cio\n\nAquisi√ß√£o de Precat√≥rios\nFunding Agressivo (emiss√£o de CDBs)\nFIDC N√£o Transparente\n\nContexto Sist√™mico e Externo\n\nAmbiente Pol√≠tico-Institucional Permissivo\nWill Bank e Voiter (riscos reputacionais)\n\nResposta Reguladora\n\nInterven√ß√£o Regulat√≥ria (Bacen atuou)\n\n\n\nEstrutura dos caminhos causais no DAG:\n\nConfounders:\n\n‚ÄúAmbiente Pol√≠tico-Institucional‚Äù afeta tanto ‚ÄúGovernan√ßa Fr√°gil‚Äù quanto ‚ÄúBacen atuou‚Äù ‚Üí necessidade de controle (via IPW).\n\nColliders:\n\n‚ÄúColapso‚Äù √© o collider final onde m√∫ltiplos caminhos convergem (governan√ßa, funding, reputa√ß√£o, interven√ß√£o).\n\nBackdoors identificados:\n\nExemplo: ‚ÄúAmbiente Pol√≠tico‚Äù ‚Üí ‚ÄúBacen atuou‚Äù ‚Üí ‚ÄúColapso‚Äù ‚Üí precisa ser bloqueado para isolar o efeito real de ‚ÄúGovernan√ßa Fr√°gil‚Äù sobre ‚ÄúColapso‚Äù.\n\nInterven√ß√µes hipot√©ticas simuladas:\n\nPresen√ßa de auditoria externa;\nAus√™ncia de aquisi√ß√£o de precat√≥rios;\nA√ß√£o precoce do regulador.\n\n\nEssas interven√ß√µes foram aplicadas via operador \\(do(X)\\), com seus impactos estimados no espa√ßo contrafactual."
  },
  {
    "objectID": "blog/master_bank_study/index.html#l√≥gica-de-ativa√ß√£o-das-vari√°veis-no-dag",
    "href": "blog/master_bank_study/index.html#l√≥gica-de-ativa√ß√£o-das-vari√°veis-no-dag",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "L√≥gica de Ativa√ß√£o das Vari√°veis no DAG",
    "text": "L√≥gica de Ativa√ß√£o das Vari√°veis no DAG\nCada n√≥ \\(X_j\\) do DAG √© ativado de acordo com a seguinte regra de decis√£o:\n\\[\nX_j =\n\\begin{cases}\n1, & \\text{se } \\sum_{i \\in pa(j)} \\theta_{ij} X_i + \\epsilon_j &gt; \\tau_j \\\\\n0, & \\text{caso contr√°rio}\n\\end{cases}\n\\]\nOnde:\n\n\\(\\theta_{ij}\\) √© o peso causal sorteado para o arco \\(i \\to j\\),\n\\(\\epsilon_j \\sim \\mathcal{N}(0, \\sigma^2)\\) √© um ru√≠do aleat√≥rio de m√©dia zero,\n\\(\\tau_j\\) √© um limiar fixo espec√≠fico para ativa√ß√£o da vari√°vel.\n\nFun√ß√µes de ativa√ß√£o utilizadas:\n\nPara vari√°veis bin√°rias simples (ex: ‚ÄúGovernan√ßa Fr√°gil‚Äù); fun√ß√£o degrau.\nPara desfechos cont√≠nuos intermedi√°rios (ex: ‚ÄúPropens√£o ao Colapso‚Äù); fun√ß√£o log√≠stica.\n\nIsso permite representar tanto eventos discretos (ativou ou n√£o ativou) quanto gradientes de risco."
  },
  {
    "objectID": "blog/master_bank_study/index.html#testes-de-sensibilidade-das-simula√ß√µes",
    "href": "blog/master_bank_study/index.html#testes-de-sensibilidade-das-simula√ß√µes",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "Testes de Sensibilidade das Simula√ß√µes",
    "text": "Testes de Sensibilidade das Simula√ß√µes\nPara garantir que as conclus√µes do modelo n√£o fossem artefatos espec√≠ficos das suposi√ß√µes iniciais, realizamos testes de sensibilidade:\n\nVaria√ß√£o das distribui√ß√µes priors:\n\n\nRodamos as 1.000 simula√ß√µes sob tr√™s tipos de distribui√ß√£o de pesos:\n\nBeta-like (cen√°rio centrado)\nGaussiana truncada (vi√©s negativo)\nUniforme truncada (incerteza m√°xima)\n\n\n\nCompara√ß√£o das taxas de colapso:\n\n\nAs frequ√™ncias de colapso foram similares entre as tr√™s distribui√ß√µes (82% a 88%), demonstrando robustez estrutural.\n\n\nRobustez do impacto marginal (ATT):\n\n\nReestimamos os ATT sob cada prior distinta, e as ordens de import√¢ncia das vari√°veis cr√≠ticas se mantiveram consistentes.\n\nEsses testes refor√ßam que o colapso do Banco Master √© estruturalmente robusto √† variabilidade param√©trica ‚Äî uma caracter√≠stica t√≠pica de sistemas fr√°geis."
  },
  {
    "objectID": "blog/master_bank_study/index.html#o-observador-como-arquiteto-dos-mundos-poss√≠veis",
    "href": "blog/master_bank_study/index.html#o-observador-como-arquiteto-dos-mundos-poss√≠veis",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "O Observador como Arquiteto dos Mundos Poss√≠veis",
    "text": "O Observador como Arquiteto dos Mundos Poss√≠veis\nEm conson√¢ncia com a cr√≠tica epistemol√≥gica de Cartwright (2007) e Hacking (1983), este estudo reconhece que:\n\nToda estrutura causal √© uma constru√ß√£o dependente do observador;\nAs escolhas de quais vari√°veis incluir, quais rela√ß√µes modelar, e quais priors utilizar s√£o atos de modelagem ativa.\n\nO observador:\n\nDefine o DAG e as distribui√ß√µes priors,\nSeleciona as interven√ß√µes contrafactuais a serem testadas,\nMas n√£o manipula os resultados: uma vez definida a arquitetura, os mundos simulados evoluem autonomamente conforme suas regras causais.\n\nEssa postura metodol√≥gica √© essencial para interpretar os resultados simulados: eles s√£o propriedades da estrutura modelada, n√£o artefatos da vontade do analista."
  },
  {
    "objectID": "blog/master_bank_study/index.html#simula√ß√£o-monte-carlo-dos-mundos-poss√≠veis",
    "href": "blog/master_bank_study/index.html#simula√ß√£o-monte-carlo-dos-mundos-poss√≠veis",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "Simula√ß√£o Monte Carlo dos Mundos Poss√≠veis",
    "text": "Simula√ß√£o Monte Carlo dos Mundos Poss√≠veis\nO processo de simula√ß√£o utilizado neste estudo pode ser formalmente caracterizado como uma simula√ß√£o Monte Carlo.\nA t√©cnica de Monte Carlo consiste em:\n\nAmostrar repetidamente valores de vari√°veis aleat√≥rias (neste caso, os pesos causais \\(\\theta_{ij}\\),\nPropagar esses valores atrav√©s de uma estrutura funcional (o DAG causal),\nObservar a distribui√ß√£o emp√≠rica dos resultados (colapso ou sobreviv√™ncia) ao longo de m√∫ltiplas repeti√ß√µes independentes.\n\nEspecificamente neste estudo:\n\nForam realizadas 1.000 repeti√ß√µes independentes,\nEm cada repeti√ß√£o, um conjunto novo de pesos causais foi sorteado conforme uma distribui√ß√£o prior definida (Beta-like, Gaussiana truncada ou Uniforme truncada),\nA evolu√ß√£o do sistema foi simulada seguindo a l√≥gica condicional da DAG, ativando vari√°veis e propagando eventos,\nO desfecho de colapso foi registrado em cada mundo.\n\nEssa abordagem permitiu:\n\nEstimar a frequ√™ncia emp√≠rica de colapso sob diferentes suposi√ß√µes de estrutura causal,\nAvaliar a robustez estrutural do sistema frente a varia√ß√µes aleat√≥rias nos pesos,\nEstimar efeitos causais m√©dios (ATT) com base em universos simulados ponderados.\n\nA utiliza√ß√£o da metodologia Monte Carlo √© particularmente adequada para contextos como este, onde:\n\nA estrutura causal √© complexa e interativa,\nAs distribui√ß√µes param√©tricas n√£o podem ser assumidas rigidamente,\nO espa√ßo de possibilidades √© vasto e n√£o-linear.\n\nO processo de simula√ß√£o realizado neste estudo caracteriza-se como uma simula√ß√£o Monte Carlo causal estruturada.\nFormalmente, o procedimento √© definido como:\n\n(1) Sorteio dos Pesos Causais \\((\\theta)\\)\nPara cada mundo \\(s \\in \\{1, 2, \\ldots, S\\}\\), sorteamos pesos causais \\(\\theta_{ij}^{(s)}\\) para cada arco causal \\(i \\to j\\) no DAG, a partir de uma distribui√ß√£o prior espec√≠fica \\(\\mathcal{P}(\\theta)\\).\n\\[\n\\theta_{ij}^{(s)} \\sim \\mathcal{P}(\\theta)\n\\]\nOnde:\n\n\\(\\mathcal{P}(\\theta)\\) pode ser uma Beta-like, Gaussiana truncada ou Uniforme truncada, dependendo do cen√°rio de simula√ß√£o.\n\n\n\n(2) Ativa√ß√£o das Vari√°veis Causais\nCada vari√°vel \\(X_j^{(s)}\\) em cada mundo \\(s\\) √© ativada conforme:\n\\[\nX_j^{(s)} =\n\\begin{cases}\n1, & \\text{se } \\sum_{i \\in pa(j)} \\theta_{ij}^{(s)} X_i^{(s)} + \\epsilon_j^{(s)} &gt; \\tau_j \\\\\n0, & \\text{caso contr√°rio}\n\\end{cases}\n\\]\nOnde:\n\n\\(pa(j)\\) √© o conjunto de pais causais de \\(j\\),\n\\(\\epsilon_j^{(s)} \\sim N(0, \\sigma^2)\\) √© ru√≠do aleat√≥rio espec√≠fico do n√≥ \\(j\\),\n\\(\\tau_j\\) √© o limiar de ativa√ß√£o da vari√°vel \\(j\\).\n\nEssa regra constr√≥i a trajet√≥ria causal dentro de cada mundo simulado.\n\n\n(3) Determina√ß√£o do Desfecho: Propens√£o ao Colapso\nO desfecho final ‚ÄúColapso‚Äù \\(Y^{(s)}\\) √© calculado como a ativa√ß√£o da propens√£o agregada:\n\\[\n\\mathbb{P}(Y^{(s)} = 1) = \\sigma \\left( \\sum_j \\gamma_j X_j^{(s)} \\right)\n\\]\nOnde:\n\n\\(\\gamma_j\\) s√£o pesos finais atribu√≠dos a cada vari√°vel cr√≠tica de risco,\n\\(\\sigma(z) = \\frac{1}{1+e^{-z}}\\) √© a fun√ß√£o log√≠stica padr√£o.\n\nO colapso √© ent√£o sorteado como uma vari√°vel Bernoulli:\n\\[\nY^{(s)} \\sim \\text{Bernoulli} \\left( \\mathbb{P}(Y^{(s)} = 1) \\right)\n\\]\n\n\n(4) Expectativa Emp√≠rica: Estimadores de Interesse\nSobre os \\(S\\) mundos simulados, estimamos:\n\nTaxa de Colapso:\n\n\\[\n\\hat{\\mathbb{P}}(Y = 1) = \\frac{1}{S} \\sum_{s=1}^{S} 1 \\left[ Y^{(s)} = 1 \\right]\n\\]\n\nImpacto Marginal (ATT):\n\nUtilizando pesos de propens√£o \\(w_i\\), o ATT estimado √©:\n\\[\n\\hat{\\tau}_{ATT} = \\frac{1}{n_t} \\sum_{i \\in D=1} w_i \\left( Y_i - \\hat{Y}_i^{(0)} \\right)\n\\]\n\nOnde:\n\n\\(D = 1\\) indica mundos onde a interven√ß√£o foi aplicada,\n\\(\\hat{Y}_i^{(0)}\\) √© o contrafactual estimado para a n√£o-interven√ß√£o.\n\n\n\n(5) Processo Iterativo de Monte Carlo\nO ciclo completo acima √© repetido \\(S = 1.000\\) vezes:\nPara cada \\(s = 1, ..., 1000 : \\theta_{ij}^{(s)} \\to X_j^{(s)} \\to Y^{(s)} \\to\\) Registra resultados\nAo final, agregamos os resultados para inferir padr√µes de colapsabilidade, robustez estrutural e efeitos causais marginais.\nEste arcabou√ßo metodol√≥gico prepara o terreno para a an√°lise dos resultados: o que emerge quando liberamos o Banco Master para evoluir, mil vezes, sob diferentes pesos, perturba√ß√µes e condi√ß√µes iniciais?\n√â essa resposta que exploraremos a seguir."
  },
  {
    "objectID": "blog/master_bank_study/index.html#colapsabilidade-a-propens√£o-estrutural-ao-colapso",
    "href": "blog/master_bank_study/index.html#colapsabilidade-a-propens√£o-estrutural-ao-colapso",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "Colapsabilidade: A Propens√£o Estrutural ao Colapso",
    "text": "Colapsabilidade: A Propens√£o Estrutural ao Colapso\nO primeiro indicador avaliado foi a colapsabilidade causal, definida como a frequ√™ncia de mundos simulados em que o Banco Master evoluiu para o colapso.\nResultados agregados:\n\n\n\nDistribui√ß√£o.de.Priors\nTaxa.de.Colapso\n\n\n\n\nBeta-like (2,2)\n85,7%\n\n\nGaussiana truncada\n88,4%\n\n\nUniforme truncada\n82,1%\n\n\n\n\n\n\nO valor m√©dio foi de 85,4%.\n\nA alta colapsabilidade, mesmo sob distribui√ß√µes amplamente distintas de pesos causais, confirma a hip√≥tese de que o sistema Master era estruturalmente fr√°gil. Seguindo a l√≥gica de Minsky (2008), a fragilidade n√£o resulta apenas de eventos externos, mas emerge da arquitetura relacional interna da institui√ß√£o.\nAdicionalmente, o resultado dialoga com a cr√≠tica de Taleb (2012): sistemas complexos que aparentam estabilidade superficial podem, na verdade, esconder padr√µes de fal√™ncia latente, s√≥ percept√≠veis por abordagens estruturais ‚Äî como a simula√ß√£o causal adotada aqui.\n\nA distribui√ß√£o das probabilidades de colapso obtidas em cada mundo simulado. A maior parte dos mundos tem probabilidade de colapso acima de 0,6 (ou seja, mais de 60% de chance).\nExiste uma concentra√ß√£o forte nas regi√µes de alta probabilidade (entre 0,7 e 1,0). A estrutura causal do Banco Master empurrava a institui√ß√£o para o colapso em quase todos os cen√°rios. N√£o era preciso um choque extremo: bastava o funcionamento normal da arquitetura fr√°gil para gerar o colapso."
  },
  {
    "objectID": "blog/master_bank_study/index.html#curva-acumulada-de-colapsos-ao-longo-da-trajet√≥ria-causal",
    "href": "blog/master_bank_study/index.html#curva-acumulada-de-colapsos-ao-longo-da-trajet√≥ria-causal",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "Curva Acumulada de Colapsos ao Longo da Trajet√≥ria Causal",
    "text": "Curva Acumulada de Colapsos ao Longo da Trajet√≥ria Causal\nA sequ√™ncia dos colapsos ao longo das ativa√ß√µes dos n√≥s do DAG revelou que:\n\n40% dos colapsos ocorreram at√© o terceiro passo (envolvendo governan√ßa e funding),\n80% dos colapsos j√° haviam ocorrido at√© a sexta ativa√ß√£o.\n\n\nEssa din√¢mica evidencia que o sistema Master carregava vulnerabilidades iniciais cr√≠ticas. Desde as primeiras ativa√ß√µes de vari√°veis como Governan√ßa Fr√°gil, Aquisi√ß√£o de Precat√≥rios e Funding Agressivo, o risco de colapso j√° se elevava drasticamente.\nConforme o modelo de instabilidade end√≥gena de Minsky, pequenas perturba√ß√µes precoces em sistemas financeiros fr√°geis t√™m efeito amplificado ao longo do tempo, gerando trajet√≥rias autocatal√≠ticas de deteriora√ß√£o.\n\nA cada etapa de ativa√ß√£o do DAG, a propor√ß√£o de mundos simulados que ainda n√£o colapsaram. Logo nas primeiras etapas, a propor√ß√£o de mundos sobreviventes cai rapidamente. Ap√≥s a terceira etapa, mais da metade dos mundos j√° haviam colapsado. A partir da sexta etapa, a queda desacelera ‚Äî sobrando poucos mundos resistentes.\nO Banco Master era altamente vulner√°vel logo nas fases iniciais de sua evolu√ß√£o causal. Mesmo sem choques extremos, o colapso era um processo estrutural quase inevit√°vel. Apenas uma pequena parcela dos mundos simulados resistiu at√© as √∫ltimas etapas."
  },
  {
    "objectID": "blog/master_bank_study/index.html#clusters-principais-de-causas-de-colapso",
    "href": "blog/master_bank_study/index.html#clusters-principais-de-causas-de-colapso",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "Clusters Principais de Causas de Colapso",
    "text": "Clusters Principais de Causas de Colapso\nA clusteriza√ß√£o das trajet√≥rias de fal√™ncia permitiu identificar tr√™s grupos dominantes:\n\n\n\nCluster.de.Colapso\nDescri√ß√£o\nFrequ√™ncia....\n\n\n\n\nGovernan√ßa-Fragilidade\nFalhas internas de controle e integridade\n37%\n\n\nJudicial-Precat√≥rios\nAlavancagem excessiva via precat√≥rios\n33%\n\n\nEstrutura-Reputacional\nQuebra de confian√ßa e funding inst√°vel\n25%\n\n\n\n\n\n\nPara cada cluster de colapso, o tempo m√©dio de sobreviv√™ncia dos mundos simulados antes do colapso. Mundos dominados pelo cluster Governan√ßa-Fragilidade tendem a colapsar mais cedo (baixas medianas de sobreviv√™ncia). J√° mundos do cluster Estrutura-Reputacional t√™m sobreviv√™ncia um pouco mais longa antes de falir.\nA fragilidade de governan√ßa impactava o sistema de forma mais precoce. Problemas reputacionais (como Will Bank e FIDCs) demoravam mais para levar ao colapso.\n\nA distribui√ß√£o de probabilidades de colapso nos mundos simulados, separada por cluster dominante.\nO cluster Governan√ßa-Fragilidade concentra mundos com probabilidades muito altas de colapso. J√° o cluster Estrutura-Reputacional tem uma distribui√ß√£o um pouco mais espalhada, indicando menor propens√£o imediata ao colapso.\nA aus√™ncia de controle interno e a fragilidade de governan√ßa foram os fatores mais perigosos no sistema. Problemas reputacionais e de precat√≥rios aumentavam o risco, mas de maneira menos aguda do que a m√° governan√ßa."
  },
  {
    "objectID": "blog/master_bank_study/index.html#impacto-marginal-das-interven√ß√µes-contrafactuais-att",
    "href": "blog/master_bank_study/index.html#impacto-marginal-das-interven√ß√µes-contrafactuais-att",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "Impacto Marginal das Interven√ß√µes Contrafactuais (ATT)",
    "text": "Impacto Marginal das Interven√ß√µes Contrafactuais (ATT)\nSimulamos tr√™s interven√ß√µes hipot√©ticas:\n\nAuditoria externa obrigat√≥ria,\nProibi√ß√£o de aquisi√ß√£o de precat√≥rios,\nInterven√ß√£o regulat√≥ria precoce.\n\nOs impactos marginais estimados foram:\n\n\n\nInterven√ß√£o.Simulada\nATT..Redu√ß√£o.na.Probabilidade.de.Colapso.\n\n\n\n\nAuditoria Externa\n-22,6 p.p.\n\n\nN√£o Aquisi√ß√£o de Precat√≥rios\n-17,4 p.p.\n\n\nA√ß√£o Reguladora Precoce\n-13,8 p.p.\n\n\n\n\n\n\n\nPara cada interven√ß√£o, a distribui√ß√£o do quanto a probabilidade de colapso foi reduzida nos mundos simulados.\nAuditoria Externa tem o maior impacto: maioria dos mundos reduziu entre 20% e 30% a chance de colapso. N√£o aquisi√ß√£o de precat√≥rios tem impacto consistente, mas um pouco menor (m√©dia de ~17% de redu√ß√£o). A√ß√£o reguladora precoce tamb√©m reduz o risco, mas com menores redu√ß√µes (~14% em m√©dia).\nA interven√ß√£o de auditoria √© a mais poderosa para reduzir o risco de colapso. Mas mesmo dentro de cada interven√ß√£o, existe variabilidade: nem todos os mundos responderam da mesma forma. O impacto das a√ß√µes depende da configura√ß√£o estrutural de cada universo."
  },
  {
    "objectID": "blog/master_bank_study/index.html#robustez-e-testes-de-sensibilidade",
    "href": "blog/master_bank_study/index.html#robustez-e-testes-de-sensibilidade",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "Robustez e Testes de Sensibilidade",
    "text": "Robustez e Testes de Sensibilidade\nRealizamos testes alterando:\n\nAs distribui√ß√µes de pesos causais,\nA ordem de ativa√ß√£o dos n√≥s,\nAs combina√ß√µes de interven√ß√µes simult√¢neas.\n\nResultados:\n\nA ordem de import√¢ncia das vari√°veis cr√≠ticas se manteve;\nA taxa de colapso variou apenas marginalmente;\nA preval√™ncia dos clusters principais foi preservada.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA figura 8 mostra a distribui√ß√£o da taxa de colapso para cada tipo de prior adotado no modelo.\nBeta-like Prior: concentra√ß√£o principal entre 83% e 88% de taxa de colapso. ‚ûî A curva √© alta e sim√©trica, mostrando uma cren√ßa moderadamente alta no risco, mas ainda com alguma variabilidade.\nGaussiana Truncada Prior: distribui√ß√£o ainda mais concentrada e estreita ‚Äî colapsos quase todos entre 86% e 90%. ‚ûî Indica uma expectativa inicial de risco muito elevado e com baixa incerteza.\nUniforme Truncada Prior: a curva √© mais larga e espalhada, com concentra√ß√µes entre 78% e 85%. ‚ûî Isso mostra que uma cren√ßa mais neutra (uniforme) aceita mais varia√ß√£o no risco, mas ainda assim concentra a maioria em altas taxas.\nMesmo com priors diferentes ‚Äî sejam enviesados ou neutros ‚Äî, todos apontavam para uma estrutura sist√™mica muito fr√°gil. Pequenas diferen√ßas no formato da distribui√ß√£o, mas nenhum prior evitava o cen√°rio de alta probabilidade de colapso.\nA figura 9 mostra a distribui√ß√£o das taxas de colapso ap√≥s atualiza√ß√£o bayesiana com os dados simulados dos mundos poss√≠veis.\nBeta-like Posterior: a curva se torna mais estreita e mais alta, focando entre 83% e 87% de colapso. ‚ûî Menor variabilidade ap√≥s ver os dados, refor√ßando a expectativa inicial de alto risco.\nBeta-like Posterior: a curva se torna mais estreita e mais alta, focando entre 83% e 87% de colapso. ‚ûî Menor variabilidade ap√≥s ver os dados, refor√ßando a expectativa inicial de alto risco.\nUniforme Truncada Posterior: o achatamento se reduz, e a maioria das probabilidades se fixa entre 80% e 85%. ‚ûî Mesmo com uma cren√ßa inicial neutra, a evid√™ncia empurrou o sistema para um cen√°rio de alta probabilidade de fal√™ncia.\nOs dados observados refor√ßam e intensificam a percep√ß√£o de risco: ‚ûî Priors mais incertos tornaram-se posteriors mais focados, todos em patamares elevados de colapso.\nA fragilidade n√£o √© uma impress√£o subjetiva dos priors: ela √© confirmada objetivamente pelos dados."
  },
  {
    "objectID": "blog/master_bank_study/index.html#discuss√£o-integrada-o-banco-master-e-a-fragilidade-end√≥gena",
    "href": "blog/master_bank_study/index.html#discuss√£o-integrada-o-banco-master-e-a-fragilidade-end√≥gena",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "Discuss√£o Integrada: O Banco Master e a Fragilidade End√≥gena",
    "text": "Discuss√£o Integrada: O Banco Master e a Fragilidade End√≥gena\nOs resultados obtidos neste estudo sugerem de maneira consistente que o colapso do Banco Master n√£o foi fruto de um evento ex√≥geno isolado, mas de uma din√¢mica estruturalmente end√≥gena.\nA an√°lise causal, somada √†s simula√ß√µes bayesianas em m√∫ltiplos universos poss√≠veis, revela que a fal√™ncia era, em grande parte, latente dentro da pr√≥pria configura√ß√£o institucional do conglomerado.\nA fragilidade end√≥gena aqui se manifesta como um ac√∫mulo gradual e interativo de vulnerabilidades:\n\nA governan√ßa fraca,\nA exposi√ß√£o agressiva a ativos judiciais (precat√≥rios),\nA rela√ß√£o t√™nue com o sistema de fiscaliza√ß√£o,\nE o enfraquecimento reputacional advindo de aquisi√ß√µes problem√°ticas.\n\nCada um desses fatores, isoladamente, poderia ter sido manej√°vel. Contudo, a intera√ß√£o cumulativa entre eles ‚Äî evidenciada nas DAGs causais e nas distribui√ß√µes de probabilidade simuladas ‚Äî criou um ecossistema onde o risco n√£o era apenas alto, mas inevitavelmente autorrefor√ßado.\nOs gr√°ficos de distribui√ß√£o das taxas de colapso, tanto sob diferentes priors quanto ap√≥s a atualiza√ß√£o bayesiana (posteriors), refor√ßam essa conclus√£o. Mesmo assumindo diferentes cren√ßas iniciais sobre o ambiente de risco (neutras, enviesadas ou conservadoras), a estrutura causal levou a taxas de colapso persistentemente elevadas, acima de 80% em praticamente todas as simula√ß√µes. A posterioriza√ß√£o ‚Äî ou seja, a atualiza√ß√£o com dados ‚Äî apenas estreitou e confirmou essa tend√™ncia, reduzindo a variabilidade, mas mantendo a tend√™ncia central de alta fragilidade.\nEssa constata√ß√£o se alinha √† literatura sobre instabilidade financeira end√≥gena (Minsky, 1986; Haldane & May, 2011), segundo a qual, em sistemas interdependentes, os riscos mais relevantes s√£o aqueles que se geram e se amplificam internamente, invis√≠veis at√© que seja tarde demais para revert√™-los.\nNo caso do Banco Master, a conformidade regulat√≥ria formal ‚Äî expressa nos CADOCs, √≠ndices de Basileia, e demais reportes obrigat√≥rios ‚Äî coexistiu com a deteriora√ß√£o estrutural do modelo de neg√≥cios. A supervis√£o regulat√≥ria, ainda que presente, mostrou-se insuficiente para neutralizar a l√≥gica interna de amplifica√ß√£o de riscos que, como mostrado em nossas simula√ß√µes, j√° estava embutida no sistema muito antes dos primeiros sinais p√∫blicos de crise.\nImportante ressaltar que este estudo n√£o atribui a responsabilidade do colapso √† omiss√£o dos reguladores. O regime prudencial brasileiro, alinhado aos acordos de Basileia, oferece diretrizes, mas n√£o substitui as escolhas estrat√©gicas feitas pelas institui√ß√µes. A liberdade de empreender no setor banc√°rio, ainda que regulada, implica tamb√©m a liberdade ‚Äî e o risco ‚Äî de tomar decis√µes ruins. Este √© o custo inevit√°vel de preservar a competitividade e a autonomia do setor financeiro.\nAssim, a hist√≥ria do Banco Master ilustra com clareza a armadilha da fragilidade end√≥gena: mesmo em um ambiente regulat√≥rio formalmente adequado, estruturas internas mal calibradas podem se degradar de forma silenciosa e autossustentada, culminando em fal√™ncias sist√™micas dificilmente evit√°veis sem interven√ß√µes corretivas profundas e tempestivas."
  },
  {
    "objectID": "blog/master_bank_study/index.html#a.1-introdu√ß√£o-ao-emaranhamento",
    "href": "blog/master_bank_study/index.html#a.1-introdu√ß√£o-ao-emaranhamento",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "A.1 Introdu√ß√£o ao Emaranhamento",
    "text": "A.1 Introdu√ß√£o ao Emaranhamento\nO conceito de emaranhamento causal deriva de analogias com a mec√¢nica qu√¢ntica, onde diferentes sistemas interagem de modo que seus estados deixam de ser independentes. Em termos causais, o emaranhamento refere-se √† interdepend√™ncia estrutural de vari√°veis: mudan√ßas em uma vari√°vel impactam, direta ou indiretamente, a probabilidade de ocorr√™ncia de eventos associados a outras vari√°veis.\nNo contexto de colapso banc√°rio, o emaranhamento ocorre quando m√∫ltiplos fatores de risco n√£o apenas coexistem, mas influenciam-se mutuamente de tal maneira que aumentam a vulnerabilidade sist√™mica de forma n√£o-linear."
  },
  {
    "objectID": "blog/master_bank_study/index.html#a.2-aplica√ß√£o-no-estudo-do-banco-master",
    "href": "blog/master_bank_study/index.html#a.2-aplica√ß√£o-no-estudo-do-banco-master",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "A.2 Aplica√ß√£o no Estudo do Banco Master",
    "text": "A.2 Aplica√ß√£o no Estudo do Banco Master\nNo modelo causal constru√≠do para o Banco Master, simulamos a intera√ß√£o de vari√°veis como:\n\nFragilidade de Governan√ßa,\nExposi√ß√£o a Precat√≥rios,\nEros√£o Reputacional,\nTiming de Interven√ß√£o Regulat√≥ria.\n\nA an√°lise de emaranhamento foi realizada ao medir correla√ß√µes estruturais entre os caminhos causais. Especificamente, observou-se como a ativa√ß√£o de um risco (por exemplo, problemas de governan√ßa) aumentava significativamente a probabilidade de ativa√ß√£o de outros riscos (como exposi√ß√£o a ativos judiciais e perda de reputa√ß√£o).\nPara medir o grau de emaranhamento, simulamos 1.000 universos poss√≠veis e calculamos o percentual de universos onde colapsos simult√¢neos de fatores ocorreram."
  },
  {
    "objectID": "blog/master_bank_study/index.html#a.3-resultados-da-an√°lise-de-emaranhamento",
    "href": "blog/master_bank_study/index.html#a.3-resultados-da-an√°lise-de-emaranhamento",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "A.3 Resultados da An√°lise de Emaranhamento",
    "text": "A.3 Resultados da An√°lise de Emaranhamento\nResultado principal:\n\nAproximadamente 47% dos mundos simulados apresentaram emaranhamento significativo entre Governan√ßa, Precat√≥rios e Reputa√ß√£o, resultando em colapsos simult√¢neos ou sequenciais r√°pidos.\nO gr√°fico a seguir ilustra a propor√ß√£o de universos com alto, m√©dio e baixo grau de emaranhamento causal."
  },
  {
    "objectID": "blog/master_bank_study/index.html#a.4-discuss√£o-cr√≠tica",
    "href": "blog/master_bank_study/index.html#a.4-discuss√£o-cr√≠tica",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "A.4 Discuss√£o Cr√≠tica",
    "text": "A.4 Discuss√£o Cr√≠tica\nA an√°lise de emaranhamento demonstra que o colapso do Banco Master n√£o deve ser interpretado como uma simples consequ√™ncia aditiva de riscos isolados.\nPelo contr√°rio, a estrutura interna da organiza√ß√£o facilitava a propaga√ß√£o de choques entre dom√≠nios distintos ‚Äî governan√ßa, judicializa√ß√£o e reputa√ß√£o ‚Äî intensificando a fragilidade do sistema.\nEste comportamento confirma a teoria da instabilidade end√≥gena:\n\nPequenos dist√∫rbios em um pilar da estrutura causavam rapidamente instabilidades em outros pilares,\nElevando a propens√£o ao colapso de maneira n√£o-linear e autorrefor√ßada.\n\n\n\n\nA propor√ß√£o dos mundos simulados conforme o n√≠vel de emaranhamento causal observado.\n47% dos universos apresentaram alto emaranhamento, ou seja, uma forte interdepend√™ncia entre os fatores de risco, levando a colapsos simult√¢neos ou fortemente conectados. 32% dos universos apresentaram m√©dio emaranhamento, com intera√ß√µes moderadas entre os fatores de risco, mas ainda vis√≠veis. Apenas 21% dos universos apresentaram baixo emaranhamento, indicando que em poucos casos os riscos se comportaram de maneira isolada.\nO modelo revela que o sistema Banco Master possu√≠a uma tend√™ncia predominante de propaga√ß√£o interna de choques. A maioria dos universos colapsou n√£o por eventos isolados, mas por rea√ß√£o em cadeia entre os fatores de risco. Isso refor√ßa a interpreta√ß√£o de que o colapso foi resultado de uma fragilidade estrutural end√≥gena, n√£o de falhas individuais desconectadas."
  },
  {
    "objectID": "blog/master_bank_study/index.html#b.1-introdu√ß√£o-√†-jump-diffusion",
    "href": "blog/master_bank_study/index.html#b.1-introdu√ß√£o-√†-jump-diffusion",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "B.1 Introdu√ß√£o √† Jump Diffusion",
    "text": "B.1 Introdu√ß√£o √† Jump Diffusion\nO modelo de Jump Diffusion foi originalmente proposto para descrever movimentos de pre√ßos financeiros que combinam:\n\nVaria√ß√µes cont√≠nuas (difus√£o) como as do movimento browniano, e\nSaltos s√∫bitos e discretos (jumps) associados a eventos inesperados.\n\nMatematicamente, ele √© representado como a combina√ß√£o de um processo estoc√°stico cont√≠nuo normal (Wiener process) com um processo de Poisson que modela os saltos. A ideia √© capturar tanto a volatilidade regular quanto choques abruptos que reconfiguram o sistema."
  },
  {
    "objectID": "blog/master_bank_study/index.html#b.2-adapta√ß√£o-conceitual-para-o-estudo-do-banco-master",
    "href": "blog/master_bank_study/index.html#b.2-adapta√ß√£o-conceitual-para-o-estudo-do-banco-master",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "B.2 Adapta√ß√£o Conceitual para o Estudo do Banco Master",
    "text": "B.2 Adapta√ß√£o Conceitual para o Estudo do Banco Master\nPara o caso do Banco Master, adaptamos a ideia de Jump Diffusion como met√°fora estrutural para o comportamento do risco de colapso:\n\nDifus√£o cont√≠nua: corresponde √† deteriora√ß√£o gradual da estrutura interna ‚Äî governan√ßa, reputa√ß√£o, exposi√ß√£o judicial.\nSaltos: representam eventos discretos e inesperados ‚Äî como crises reputacionais, inadimpl√™ncia de precat√≥rios, mudan√ßas abruptas na liquidez de FIDCs.\n\nAssim, o colapso do Banco Master pode ser interpretado n√£o como um processo puramente cont√≠nuo ou previs√≠vel, mas como a evolu√ß√£o de um sistema com fragiliza√ß√£o progressiva, interrompida por eventos cr√≠ticos que aceleraram a ruptura."
  },
  {
    "objectID": "blog/master_bank_study/index.html#b.3-resultados-simulados-e-interpreta√ß√£o",
    "href": "blog/master_bank_study/index.html#b.3-resultados-simulados-e-interpreta√ß√£o",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "B.3 Resultados Simulados e Interpreta√ß√£o",
    "text": "B.3 Resultados Simulados e Interpreta√ß√£o\nA partir da simula√ß√£o dos 1.000 universos paralelos, observamos que:\n\nEm 68% dos mundos simulados que colapsaram, houve uma acelera√ß√£o abrupta na sequ√™ncia causal ‚Äî equivalente a um salto (‚Äújump‚Äù) que rompeu a estabilidade gradual.\nEm apenas 32% dos mundos, o colapso se deu por deteriora√ß√£o cont√≠nua sem saltos relevantes.\n\n\n\n\nA linha cinza tracejada representa uma deteriora√ß√£o cont√≠nua e gradual (processo difusivo). A linha vermelha mostra o processo real com saltos (jumps) ‚Äî momentos em que eventos cr√≠ticos causam acelera√ß√µes abruptas no colapso. Os pontos pretos marcam os momentos dos saltos inesperados."
  },
  {
    "objectID": "blog/master_bank_study/index.html#b.4-implica√ß√µes-para-a-an√°lise-de-colapso",
    "href": "blog/master_bank_study/index.html#b.4-implica√ß√µes-para-a-an√°lise-de-colapso",
    "title": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso",
    "section": "B.4 Implica√ß√µes para a An√°lise de Colapso",
    "text": "B.4 Implica√ß√µes para a An√°lise de Colapso\nA presen√ßa predominante de ‚Äúsaltos‚Äù na din√¢mica de colapso sugere que estrat√©gias tradicionais de monitoramento cont√≠nuo (difus√£o) s√£o insuficientes.\nSistemas como o do Banco Master exigem modelos de alerta capazes de detectar sinais precoces de descontinuidade:\n\nMudan√ßas abruptas na percep√ß√£o de cr√©dito,\nQuebras de confian√ßa sist√™mica,\nFluxos at√≠picos em instrumentos como FIDCs e precat√≥rios.\n\nEssa conclus√£o refor√ßa a necessidade de incorporar an√°lises de risco n√£o-lineares e descont√≠nuas nos mecanismos de supervis√£o prudencial, especialmente em conglomerados que operam no limite da toler√¢ncia regulat√≥ria."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Mil Contrafactuais: o caso do Banco Master no Multiverso do Colapso\n\n\n\n\n\n\nRisco\n\n\ninfer√™ncia contrafactual\n\n\nredes bayesianas\n\n\ncolapso banc√°rio\n\n\nsupervis√£o financeira\n\n\nsimula√ß√µes\n\n\nmultiverso\n\n\n\n\n\n\nApr 20, 2025\n\n\nHenrique Costa\n\n\n\n\n\n\n\n\n\n\n\n\nFinancial Cycle: Credit - Part I\n\n\n\n\n\n\nRisk\n\n\nCredit\n\n\nCycle\n\n\nDataViz\n\n\n\n\n\n\nOct 19, 2024\n\n\nHenrique Costa\n\n\n\n\n\n\n\n\n\n\n\n\nTrivia Series: Modelling foundations\n\n\n\n\n\n\nMaths\n\n\nStats\n\n\nModel\n\n\n\n\n\n\nAug 25, 2024\n\n\nHenrique Costa\n\n\n\n\n\n\n\n\n\n\n\n\nSorte e Apostas\n\n\n\n\n\n\nTomada de Decis√£o\n\n\nRiscos\n\n\n\n\n\n\nApr 16, 2024\n\n\nHenrique Costa\n\n\n\n\n\n\n\n\n\n\n\n\nTrivia Series: Regression\n\n\n\n\n\n\nR\n\n\nDataViz\n\n\n\n\n\n\nDec 20, 2023\n\n\nHenrique Costa\n\n\n\n\n\n\n\n\n\n\n\n\nA few words about risk management\n\n\n\n\n\n\nManagement\n\n\nRisk\n\n\nFinance\n\n\n\n\n\n\nAug 27, 2023\n\n\nHenrique Costa\n\n\n\n\n\n\n\n\n\n\n\n\nFor me, the choice was R or Python\n\n\n\n\n\n\nR\n\n\nPython\n\n\n\n\n\n\nJun 20, 2023\n\n\nHenrique Costa\n\n\n\n\n\n\n\n\n\n\n\n\nReproduzindo gr√°fico do Economista Visual - Valor da cesta b√°sica (PT-BR)\n\n\n\n\n\n\nR\n\n\nDataViz\n\n\n\n\n\n\nFeb 20, 2021\n\n\nHenrique Costa\n\n\n\n\n\n\n\n\n\n\n\n\nCan Economists Become Data Scientists?\n\n\n\n\n\n\nEconomics\n\n\nTheory\n\n\nData Science\n\n\nEconometrics\n\n\n\n\n\n\nOct 29, 2018\n\n\nHenrique Costa\n\n\n\n\n\n\n\n\n\n\n\n\nThe mathematization of Economics\n\n\n\n\n\n\nEconomics\n\n\nTheory\n\n\nMathematics\n\n\n\n\n\n\nSep 21, 2018\n\n\nHenrique Costa\n\n\n\n\n\n\n\n\n\n\n\n\nThe Ceteris Paribus Condition\n\n\n\n\n\n\nEconomics\n\n\nTheory\n\n\n\n\n\n\nSep 16, 2018\n\n\nHenrique Costa\n\n\n\n\n\n\nNo matching items\n\n\n  \n\nReuseCC BY 4.0CitationFor attribution, please cite this work as:\n‚ÄúBlog Posts.‚Äù n.d. https://henriqueqrm.netlify.app/blog/."
  },
  {
    "objectID": "blog/financial_cycle_credit_01/index.html",
    "href": "blog/financial_cycle_credit_01/index.html",
    "title": "Financial Cycle: Credit - Part I",
    "section": "",
    "text": "After a really cool chat over coffee with my friend Catarina and some discussions on the topic of cycles and time series with my friends Emanuella and Patr√≠cia, I thought about writing an article here for this blog and resurrecting it, updating and improving an analysis I did some time ago and published on LinkedIn, which can be accessed through this link: Credit Granting can be used as a proxy for the Financial Cycle, which tends to present a ‚Äúboom‚Äù before the beginning of a recession.\nBased on the study ‚ÄúThe Financial Cycle in Brazil‚Äù carried out in the 3rd Project of the 2018 Agreement between FEBRABAN (Brazilian Federation of Banks) and PUC - Rio (Pontifical Catholic University of Rio de Janeiro), where the authors describe the five main stylized characteristics of the financial cycle based on the work of Borio (2012), which are:\nIn this paper, we will focus on seeing only part of the first characteristic, in which the financial cycle is described in a more parsimonious way in terms of credit.\nIf you want me to develop part 2 onwards, analyzing the other characteristics, leave a comment here or in the LinkedIn post at this link"
  },
  {
    "objectID": "blog/financial_cycle_credit_01/index.html#visualizing-gaps",
    "href": "blog/financial_cycle_credit_01/index.html#visualizing-gaps",
    "title": "Financial Cycle: Credit - Part I",
    "section": "Visualizing gaps",
    "text": "Visualizing gaps\n\nTo conclude this first article, let‚Äôs look at the graph of the financial gap through the credit cycle estimated by the HP filter and the CEEMDAN algorithm.\nFor educational purposes, the gap is the difference between potential credit and effective credit.\n\n\n\n\n\n\n\n\n\n\n\nTell me in the comments what you think of this chart. Which cycle best captures business cycles?\nTo conclude, I tried to explore the Harding & Pagan (2002) algorithm for dating cycles. In short, I applied it to the total credit granting series, to try to find short-term cycles.\n\n\n\n\n\n\n\n\n\n\n\nIf you are unable to comment here, you can go to the updated post I made on LinkedIn about this article AT THIS LINK HERE. I would really appreciate it if you would like to contribute to the text, or if you have any corrections or adjustments to suggest to me. Thank you very much for your time.\n\n\n\n\n\n\n\nHey! üëã, did you find my work useful? Consider buying me a coffee ‚òï, by clicking here üëáüèª"
  },
  {
    "objectID": "blog/r_or_python/index.html",
    "href": "blog/r_or_python/index.html",
    "title": "For me, the choice was R or Python",
    "section": "",
    "text": "Having decided to improve my skills in the short space of time left between my professional life and personal life, I started researching the available data science options. It quickly became clear that I should choose a programming language to learn and apply the techniques and the choice fell on R or Python, for me both options are good because they are free and open source and have an excellent reputation, in addition to each one having its own own community of active users, and very active!\nI don‚Äôt remember where I saw this tip, but for me it was the central factor in my decision and something worth sharing:\nIf you are a professional who uses analysis tools in a business context, and with experience in Microsoft Excel, my tip is: select R to start your journey in data science. R is a single-threaded object-oriented functional programming language, and once you understand the main commands, it is intuitive to use. As such, it‚Äôs pretty predictable, which has been great for me as a professional. R has an excellent graphics package that complements the idea of analyzing data or data wrangling, which Python programmers often turn to ggplot2 to generate beautiful visualizations, or use the ggplot2 theme within Python (lol), even though in recent years the Python‚Äôs visualization libraries have improved a lot, yet this happens frequently.\nOn the other hand, if you already have some experience in programming, or are from the computing field, Python is a much more general purpose and more readable language for those coming from this field, and everything you can do in Python is the same as most of the things R is good at.\nBoth groups have strong communities that share their knowledge on various blogs and events and I really think they are both great choices. Today the market is more heated when it comes to Python, and professionals who know both languages do very well.\nNow that I‚Äôve been using R for a few years (since 2013), I‚Äôve had the chance to use it a few times in the workplace, and I use it whenever I can, showing the resources this tool offers, creating econometric models and estimation procedures. in modeling for financial risks to provide some business insights. I have already carried out many analysis projects for private clients and I feel honored that my efforts to learn are generating results.\nA fellow Python expert with an interest in machine learning once saw the few lines of code needed to organize the dataset, train a model, and predict results, and, frankly, he was shocked. We had an awkward R-to-Python conversation trying to understand the differences between data matrices and data frames afterwards, although for me it clarified where R was strong: the ease of preparing and building a model quickly for any type of analysis.\nBut depending on who you ask, when I first came into contact with R in 2013, R probably had a slight, if not substantial, advantage over Python in user adoption for machine learning and what is now known as data science. Since then, the use of Python has grown substantially and it would be difficult to argue against the fact that Python is the new favorite, although the race may be tighter than one might expect given the enthusiasm of Python fans supporting the new and Brilliant tool with biggest hype.\nIn recent years, Python has benefited greatly from the rapid maturation of free add-ons such as the Scikit-learn machine learning framework, the Pandas data structure library, the Matplotlib graphing library, and the Jupyter notebook interface, among several other open applications. Source libraries that make it easier than ever to do data science in Python. Of course, these libraries only brought Python to par with what R and RStudio could do long ago! However, Python is comparatively fast and memory efficient ‚Äì at least relative to R ‚Äì which may have contributed to the fact that Python is now arguably the language most frequently taught in formal programs in data science. and quickly gained adoption across business domains.\nRather than indicating the imminent death of R, because Python‚Äôs rise is steep. In fact, the use of R is also growing rapidly, and R and RStudio are becoming more popular than ever. Although students sometimes ask whether it‚Äôs worth starting with R rather than jumping straight to Python, there are still many good reasons to choose to learn machine learning with R over the alternative.\nPlease note that these justifications are quite subjective ‚Äì not just mine, but any justification on the internet will be like this ‚Äì and there is no right answer for everyone, so I hesitate to put this in writing! However, as someone who still uses R almost daily as part of my work for a large corporation, here are a few things I‚Äôve noticed:\n\nAs I mentioned above, R may be more intuitive and easier to learn for people with a background in social sciences or business (such as economics, marketing, and so on), while Python may make more sense for computer scientists and other types of engineers .\nR tends to be used more like a ‚Äúcalculator‚Äù where you type a command and something happens; In general, coding in Python tends to require more consideration of loops and other program flow commands (this distinction is disappearing over time with additional functionality in Python libraries).\nR uses relatively few types of data structures (those included are adapted for data analysis) and the frequently used spreadsheet-type data format is a built-in data type; Comparatively, Python has many specialized data structures and uses libraries like NumPy or Pandas , and for array data format, each has its own syntax.\nR and its packages can be easier to install and update than Python, in part because Python is managed by some operating systems by default, and maintaining separate dependencies and environments is a challenge (modern Python installation tools and managers of packages addressed this simultaneously, in some ways making the problem worse!).\nR is typically slower and more memory-intensive than Python for data manipulation and iteration over large data structures, but if the data fits in memory, this difference is somewhat insignificant. For real? If you don‚Äôt work with BigData, you probably don‚Äôt even feel this difference; R has improved in this area, R is making data preparation faster and easier, and for data that doesn‚Äôt fit in memory there are workarounds (R with BigData), but this is admittedly one of Python‚Äôs biggest advantages.\nR has the support and vision of the Posit team (formerly known as RStudio) driving innovation and making R easier and more enjoyable to use in a unified software environment (RStudio Desktop); In contrast, Python‚Äôs innovations are occurring on multiple fronts, offering more ‚Äúright‚Äù ways to accomplish the same thing (for better or worse).\n\nHopefully, the reasons above will give you the confidence to begin your journey. There is no shame in starting with R (as some people think), whether you stay with R long-term, use it side-by-side with other languages like Python, or major in something entirely different, the fundamental principles you learn will be transferred to any language (a clear example is: if you haven‚Äôt yet learned SQL and already understand the basics of Tidyverse, you will have an easier time studying SQL, or vice versa) or tools you choose. Although code written in R is much ‚Äúprettier‚Äù to me, I highly recommend that you use the right tool for the job, whatever it may be. You may find, as I did, that R and RStudio are your tools of choice for many real-world data science and machine learning projects - even if you occasionally take advantage of Python‚Äôs unique strengths!\nIn the end, my recommendation is: just pick one and get started. Although I will still need to learn a lot more about Python to get the unique benefits each language offers, amplified through collaboration. The good thing about this is that, using RStudio, an IDE for R, it is possible to use Python and SQL as well.\n\n\n\n\n\n\nHey! üëã, did you find my work useful? Consider buying me a coffee ‚òï, by clicking here üëáüèª\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationFor attribution, please cite this work as:\nCosta, Henrique. 2023. ‚ÄúFor Me, the Choice Was R or\nPython.‚Äù June 20, 2023. https://doi.org/10.59350/83vew-1de35."
  },
  {
    "objectID": "blog/ceteris_paribus/index.html",
    "href": "blog/ceteris_paribus/index.html",
    "title": "The Ceteris Paribus Condition",
    "section": "",
    "text": "I will present here some personal reflections on a concept that is considered very simple and perhaps common in economics, but which is labeled as the height of ignorance for making an assumption that cannot be observed in reality, but which in my view is a mistake. mistake. The concept: ceteris paribus, or better said ‚Äúthe ceteris paribus condition‚Äù.\nWell, have you ever tried to imagine the meaning of ceteris paribus? The idea is somewhat simplistic, and means ‚Äúeverything else is constant‚Äù or ‚Äúall other things remaining unchanged‚Äù. Nothing new on earth, because economists, students and enthusiasts know this condition well, and learn in the academic world that in economic analysis there is always this ceteris paribus thing.\nExamining a fact under the condition that an effect X, ceteris paribus had a cause Y, or better said, that a specific effect occurs from an isolated cause without other potential causes interfering or not have no involvement.\nIf you think about this statement in depth, this condition seems a bit absurd, as in reality a specific effect never has an isolated cause, as it is a static look at dynamic events. There are many criticisms about this assumption, but most are based on a misinterpretation of the ceteris paribus condition.\nUnderstanding the literal meaning of the term does not make you an expert user of it. It is a fact that observing or studying something and having to say that if everything else is constant, it goes beyond the idea that the results may be unrealistic. However, you need to abstract to understand. Is it possible for this condition to happen? As? And when does it occur?\nIf you want to carry out a methodologically in-depth study to understand scientific applications, please consume Chapter 2 of this masterpiece:\n\nSchlicht, E. (1985). Isolation and Aggregation in Economics. [S.l.]: Springer Verlag. ISBN 0-387-15254-7.\n\nTo illustrate, I will simulate a situation and use an example where I prepare my morning coffee, and so by assumption, I will sweeten it (as sugar is not welcome in my coffee). So I have the following ingredients: water, sugar, and 100% Arabica coffee powder (my favorite). We know that to prepare coffee, roughly speaking, you just need to mix all the ingredients. Of course, as long as certain ingredients are suitable for this, as in the case of water, which must reach a preparation temperature (considered ideal) close to 90¬∞C, but there is always that classic doubt: how much sugar would be ideal for sweetening? My coffee?\nIn econometric models there is a similar methodology: to estimate a variable Y it is necessary to mix the ‚Äúeffects‚Äù of variables X, as long as some variables X meet certain conditions, as there will be moments in which it will be necessary to apply transformations to the variables, changing from level to logarithmic, applying differences, among others. As is the process of changing the water temperature.\nTo solve this problem, a common economist will say:\n\nHmm, Depends!\n\nBut a Scientist will say:\n\nFor an ex-ante analysis, it will be necessary to test hypotheses!\n\nFirstly, we outline two hypotheses that 1 liter of coffee will be prepared, and so 1 liter of water (at a temperature of 90¬∞C), 4 spoons of coffee powder (as I like strong coffee) are used, so we can say the following :\n\nAs an alternative hypothesis, the sugar value is zero, supposedly the presence of sugar in the drink is practically zero, so the drink ‚Äúneeds to be sweetened‚Äù.\nAs a null hypothesis, the sugar value is non-zero, where at least one spoon (or cube) of sugar is supposedly added, as we are curious about the positive marginal effects, so the drink is being sweetened.\n\nWe are interested in inferring about the null hypothesis, as it is from there that we consider sweetening the drink, once the alternative hypothesis occurs, simply apply the variable sugar (for a better fit of the model ) until the drink is sweetened.\nNow we can identify the marginal effects that the increase in sugar can have on the drink, and a very curious irony is that we are here looking for an optimization solution. Testing the sugar variation in order to seek an optimal point of balance, that is, where the addition of sugar can lead to a point of balance between sucrose and the acidity of the coffee, making the drink sweetened to the consumer‚Äôs taste.\nTheoretically, you add a spoonful of sugar and try it; If it‚Äôs not at the desired point, you can add a little more and try again; and so on until you reach the flavor you like most. In other words, after mixing the ingredients, only the amount of sugar will be adjusted, keeping the proportions or quantities of the other ingredients fixed. It is in this ‚Äúkeeping fixed‚Äù that the Ceteris Paribus condition occurs, that is, ‚Äúkeeping all other things unchanged‚Äù.\nThis is exactly what the ceteris paribus condition means: keeping some variables fixed in their proportions and quantities or constants (in this case, the water and coffee powder after mixing and infusing) and changing only the sugar, to understand the effect that sugar has on the variable Y (the coffee itself).\nOne of the first appearances of the use of this term in economics is contained in the work of the 17th century economist Richard Cantillon, with the title:\n\nCantillon, Richard. Essays on the Nature of Commerce in General. Routledge, 2017.\n\nIn economic science, this concept is widely used in models, mainly in neoclassical theory, as it allows the interpretation of cause and effect events in the real world in a representative way.\nImagine having to deal with the enormous and growing flow of information that is generated every day, at all times? Everyone would go crazy!!! So, economic theory simplifies the analysis so that it is possible to find solutions to problems: it is like disregarding some information (or considering that they are constant, that they do not change or do not alter your analysis) and then analyze the effects of other information.\nDo not think that this is absurd, or that it is unrealistic, if for any moment this thought crossed your mind, please review it immediately, as economic theory is made up of many, many observations, deductions and tests of the events of cause and effect of the real world, and that is why it is recognized as science today.\nAnother very interesting example in which I was able to observe the effect of this condition is in the financial market, the theory of portfolio diversification guarantees the reduction of the investor‚Äôs exposure to non-systemic risk, that is, disregarding non-systemic risk and keeping it fixed close to null may be an effect of the ceteris paribus condition, as the investor diversifies his investments to protect himself from the fluctuation of such assets, trying to keep non-systemic risk to the minimum possible, and when a correlation closer to zero is achieved, he assumes that it is free of non-systemic risk and thus alters its market positions based on systemic risks. There are so many real examples that use this condition that it goes beyond sweetening my coffee.\nThe condition ceteris paribus is present in our daily lives, in almost everything it is possible to find it, but there will be skeptics who will say: ‚Äúbut in reality nothing remains fixed or constant‚Äù, or even ‚Äúthat is part of the idea of general balance, and in general it is unreal, because balance is not empirical‚Äù, well, the real world is dynamic, but in my humble opinion, the world has always been in a natural balance, the dynamism of the real world is nothing more is than equilibria interacting simultaneously, so therefore, the ceteris paribus condition can make some real and palpable sense.\nFinally, the ceteris paribus condition means that we can keep some variables fixed or constant to analyze only what matters; This condition is very useful for creating econometric models used to better calculate and interpret real-world phenomena.\nI hope I have fulfilled my mission of exploring the ceteris paribus condition in a similar way for a better understanding. In the next publications I will deal with models in economic science, to explain why this science is so full of mathematical and statistical applications.\nAdapted from original.\n\n\n\n\n\n\nHey! üëã, did you find my work useful? Consider buying me a coffee ‚òï, by clicking here üëáüèª\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationFor attribution, please cite this work as:\nCosta, Henrique. 2018. ‚ÄúThe Ceteris Paribus Condition.‚Äù\nSeptember 16, 2018. https://doi.org/10.59350/jxf43-hc691."
  },
  {
    "objectID": "blog/economists_in_data_science/index.html",
    "href": "blog/economists_in_data_science/index.html",
    "title": "Can Economists Become Data Scientists?",
    "section": "",
    "text": "When it comes to job titles, data scientist is one of the biggest buzzwords in recent years. It is also one of the fastest growing fields of professional activity. Can an economist really be a data scientist? What skills are needed?\nExposed as the ‚Äúsexiest job of the 21st century‚Äù by Harvard Business Review, here I will discuss the reasons why an economist proclaims himself as a Data Scientist, and that goes far beyond hype.\nData Scientist, this is the new field of activity that everyone is talking about these days, and that almost everyone wants to be part of it now. It is certainly a very interesting field of professional activity, as it was born from the combination of multidisciplinary and complementary techniques, which did not require much scientific formalism and academic consolidation to be called a Science or Profession. Why?\nThe term and the field of activity have existed for a long time, but thinking about the hype that perhaps it is for a simple reason: the first to give themselves the luxury of calling themselves Data Scientist, were already excellent professionals in the techniques they corroborated for its creation, that is, they were already qualified professionals in certain areas that today have been shaped as necessary techniques to be a Data Scientist.\nIt is not particularly easy to define Data Science as a whole or in main ‚Äúnecessary‚Äù techniques, but here I want to talk about why an economist can proclaim himself as a ‚ÄúData Scientist‚Äù.\nAs some people may know, on LinkedIn or elsewhere (social media), some professionals call themselves Data Scientists. But why? How did these people become data scientists? Why after studying Physics, Computer Engineering, Mathematics, Statistics, Economics or any other professional training are now suddenly part of this field? This is among other questions that many ask."
  },
  {
    "objectID": "blog/economists_in_data_science/index.html#why-can-an-economist-be-a-data-scientist",
    "href": "blog/economists_in_data_science/index.html#why-can-an-economist-be-a-data-scientist",
    "title": "Can Economists Become Data Scientists?",
    "section": "Why can an economist be a data scientist?",
    "text": "Why can an economist be a data scientist?\nThe great thing here is that it is not necessary to study this in a ‚Äúformal school‚Äù, such as professional courses, technologists or a university degree (of course, nowadays there are all these things, even postgraduate programs focused on this area , but as we are living in the information age, where a cascade of information is available on the internet and freely accessible, the need to have a certificate or diploma is almost nil), so why should I, as an economist, have the right to call me a Data Scientist?\nIf you think about it, we are used to following the path where you graduate in a specific field like Economics, so you have the right to call yourself an Economist, but what happens when a new field is developing, and you are a part of it? of this growth, and also there was no specific degree or postgraduate degree to become a part of this field?\nEconomists have a set of skills that can make them successful in this area. Economists have extensive training in articulating complex ideas, something that students in other disciplines often may not have, such as business sense where the value generated is business insight.\nWe can ask Economics students a fuzzy question or problem and they answer it with priori and posteriori analysis based on information (data), and then convert it back into understandable words that a non-economist could understand . This is a very important skill to become a Data Scientist, and one that professionals lack.\nMost Data Scientists do not approach problems like Economists do, when they carry out their studies and analyzes using econometrics. In Data Science there is no unifying theory, the objective is to predict the results of the data, the approach has its merits, and predictions prevail in the industry.\nHowever, your training as an economist will help you avoid drawing some inappropriate conclusions from the data, as many data scientists don‚Äôt have the feeling for how deep structural changes can undermine predictions.\nBut I want to suggest here that economics is ‚Äî surprisingly ‚Äî a great foundation for Data Science.\nYes! Yes! Yes! Please give me a chance to explain further. I know I‚Äôm biased, but I believe there aren‚Äôt many courses that will give you better training to work in Data Science than economics."
  },
  {
    "objectID": "blog/economists_in_data_science/index.html#ok-and-so-is-an-economist-a-data-scientist-or-not-how-it-works",
    "href": "blog/economists_in_data_science/index.html#ok-and-so-is-an-economist-a-data-scientist-or-not-how-it-works",
    "title": "Can Economists Become Data Scientists?",
    "section": "OK! And so? Is an Economist a Data Scientist or not? How it works?",
    "text": "OK! And so? Is an Economist a Data Scientist or not? How it works?\nLooking closely at the descriptions of common positions in Data Science, and the range of subjects at universities that offer undergraduate courses in economics, one can quickly deduce that economics would not be the best training to have.\nBecause most economics programs don‚Äôt teach programming languages and databases, not even about projects. What the hell is this R guy? Python? And what about Hadoop? And there‚Äôs still Hive and Pig? And now there‚Äôs TensorFlow. This has to be a joke!\nSpecific skills such as programming and databases are not included in the curriculum or the most important, however, studying Economics can provide a framework that will allow you to learn specific skills quickly. And a good economic education is indeed a solid background to have.\nThere are professionals who defend this thesis, as is the case of V√≠tor Wilher, who, in addition to having a master‚Äôs degree in economics and responsible for the website An√°lise Macro, He is also a teacher of several programming courses in the R language and data analysis.\nThis discussion about the importance of knowing how to program a language as a tool that offers an excellent relationship between analytical capacity, data collection and presentation, as well as the potential that this can offer for students and professionals, especially for young people at the beginning of their careers in Economy.\nSome reasons that Economists make great Data Scientists, and that no one tells them:\n\nEconomist already knows machine learning!\nBefore you think about stopping reading, thinking that this article is already ‚Äútravelling‚Äù or that the writer must have gone to a very strange economics college to learn about machine learning, but be careful:\nMachine learning is really just a fancy word for statistical and predictive modeling that programmers invented to make their business look better, get more attention, and even keep non-participants out of their club. Maybe they should know something about economics, after all ‚Äî scarcity raises prices! (laughter).\n\nA well-observed fact is that the first two modules of a machine learning course (I‚Äôm commenting on the most popular one on the Coursera website) are linear regression and logistic regression. (sarcastic laughter)\nWell, 99.99% of economists who took an introductory econometrics course, this may surprise you, but these economists probably have a deeper knowledge of linear regression than a junior or full-time data scientist.\nJust as it can be scary to come across names like ‚Äúneural networks‚Äù or ‚Äúsupport vector machines ‚Äî SVM‚Äù, the economist would possibly have to work very hard, even break a sweat to find the term ‚Äúheteroscedasticity‚Äù anywhere in machine learning programs.\nTo learn more, access these guides:\n\nHacker‚Äôs guide to Neural Networks\nA Neural Network in 11 lines of Python\n\nBut of course, neural networks can be a very deep field, much deeper than the way it has been described. Just like Recurrent nets, convolutional nets, deep learning are much more advanced and complex topics ‚Äî and their algorithms are much more powerful.\nBut for most machine learning applications, an economist should do just fine with simple models: basic neural networks, binary decision trees, regressions, SVMs. And with the statistical basis of most economics courses and econometric applications, you will have no problem understanding these concepts quickly.\n\n\nEconomists have higher standards\nCan you recite all the basic assumptions of the OLS method? What about all the possible threats to the internal and external validity of your model that could compromise your analysis?\nOf course you can, I know you are nerds. (hahaha)\nAt least in my experience as an academic, the discipline of econometrics was temporarily obsessed with finding causal relationships‚Äîand making it very clear how difficult this phenomenon is to observe without randomized controlled trials.\nNot to mention that most models are sensitive to their own basic assumptions. A serious talk would not end without someone mentioning another possible source of bias, attenuation bias, survival bias, selection bias, measurement error, reverse causality, truncation, censoring, omission, spurious correlation, etc.\nFor each problem, there was another model ‚Äî even more complicated ‚Äî to deal with it. A model that could also introduce its own baggage of assumptions and problems. The world of econometrics became confusing and more nebulous as the disciplines advanced, in addition to creating the impression of being uncertain and frustratingly limiting. Then Artificial Intelligence, Machine Learning and Data Science emerged to illuminate this dark path.\nWarning: gross exaggeration ahead.\n\nCompared to all this, machine learning is wonderfully, charmingly simpler. Instead of solving models explicitly ‚Äî based on strict assumptions ‚Äî they are estimated iteratively with the gradient method (and its derivatives). Rather than testing or validating the theory behind the event you are trying to study, and carefully selecting explanatory variables and the appropriate model, you can try everything you can think of and see if the answer holds up.\n\n\n\nAlbert Einstein: ‚ÄúInsanity is continuing to do the same thing over and over again and expecting different results.‚Äù Machine Learning:\n\n\nGet used to cross-validation and testing, instead of t-statistics, why not try some bootstrapping? And talking about bootstrapping, there are already some studies going on the internet criticizing the use of this technique, but while the discussion does not consolidate, we will continue to use it.\nFor economists who are enthusiastic about econometrics, this may seem pure blasphemy. But this is only because the expectation is high of finding the same ML that was expected in econometrics. Inference and causal interpretation. However, most of the time, ML is aimed at predicting and finding patterns, not causality. For some models, you can‚Äôt simply say which variables are most important in predicting outcomes.\nAnd yes! Unfortunately (I bring some truths) neural networks cannot be used to explain the causal effect of the minimum wage on unemployment. But Mr.¬†Economist (who runs models) also cannot expect a model like logit (multinomial) to be used to recognize handwriting. What I want to say here is all about the correct use of the right tools in their applications ‚Äî and I‚Äôm sure econometrics teaches you very well about this.\n\n\nEconomists really know how to write coherent reports!\nIn data science it‚Äôs not just about fancy algorithms, however, unless you‚Äôre an academic researcher who just writes theoretical papers (an isolated case, and if, only if, it‚Äôs true, you probably wouldn‚Äôt be reading this anyway). , the presentation of results and writing in a simple, concise and coherent way are present in economics.\nIf an economist works as a data scientist anywhere in the ‚Äúreal world‚Äù, and will have to present his results to non-technical audiences ‚Äî managers, marketers and writers, and clients ‚Äî he will have to be able to show why your results are important and how normal people can use and act on them.\nAs an economist, I bet that most economists wrote their fair share of articles, essays, reports, presentations and dissertations and theses in their temples ‚Äî that little work or study room, gloomy and only inhabited by beings of their own species ‚Äì - at university using MS Excel, perhaps a GRTL, the bravest E-Views, or even those outliers who venture into distant lands using * Stata*.\nDon‚Äôt underestimate this skill. It might generate some comments about how archaic this is, but the fact is that probably having this skill puts the economist well ahead of most computer scientists and mathematicians, statisticians or any other professional when it comes to generating robust analyses, presenting and explaining your work clearly ‚Äî and bringing together longer texts that have structures and logic behind them (at least that‚Äôs what should happen, now whether that actually happens I don‚Äôt know, I‚Äôll let this curiosity in the air).\n\n\nLearning programming is not difficult\nUnfortunately, to be a data scientist you will probably have to write code scripts. But not excluding the fact that economists did not need to program either. It is true that using Stata can be seen as ‚Äúprogramming‚Äù, however, it is not a ‚Äúproper‚Äù programming language, but it is a great introduction for those starting out in statistical computing. And if there is a possibility to continue to graduate school, many economics programs use other languages‚ÄîPython is very common, as are R and Matlab.\nTo the delight of some, Python has become the ‚Äúlingua franca‚Äù of data science, perhaps because it is a generalist language, in addition to being a very readable and easy-to-learn language. But I particularly like R (oops! Preference revealed, successfully detected), as it not only has a large selection of libraries, but also has a widely active community, in addition to be built precisely for this purpose.\nThe preference for the R language is because it is also powerful, but the syntax is seen as an ‚ÄúABOMINATION‚Äù by programmers of other languages. Matlab is commercial software, and although it is great (and fast) at mathematical computing, and also has an open source alternative (Octave), it is not that common. Julia is a very obscure language and still a little too young to be considered a language that would be well suited to the activities of a data scientist, but it is known so far that users here in Brazil are increasing, even some professionals at the Central Bank of Brazil already use Julia.\n\n\nSo why doesn‚Äôt anyone tell you this?\nUltimately, economists should declare themselves as great data scientists, or at least own that term, or do as little as possible to acquire it. But then why doesn‚Äôt anyone at university tell them that this is a ‚Äúreal world‚Äù career choice? On the one hand, one of the reasons is that everything is relatively new. And course structures are slow to change, and how long it takes ‚Äî favoring more traditional options in finance, academia, government.\nIn fact, the college I studied at has a professor, better known as Roney who is striving to introduce this change, and add this to the training curriculum of undergraduate students, in addition to others such as professor Adriano which has also adopted the use of these programming languages in teaching economics (to be more precise, in time series econometrics), but as said: the process is slow.\nBut don‚Äôt think that economists can‚Äôt act as data scientists in these areas mentioned (finance, academia, government), quite the contrary, it‚Äôs becoming more common every day. But I also think that there is still a bit of prejudice (or perhaps, a bit of fear) in the economic world against data science, as they defend the thesis that an economist entering into data science is beneath the main cause, as they are concerned with bigger issues.\nI‚Äôm just sorry, because it‚Äôs a shame. Because economics gives its graduates a unique blend of (statistical) and soft (human) skills that are much harder to find in Mathematics departments. Computer Science, Statistics and others.\nAnd perhaps data science roles only benefit from having careful economists (econometric enthusiasts) doing the work. This way, econometricians can make the best use of ML when it comes to testing and cross-validation and algorithmic estimation approaches.\nSo give yourself a chance and get to know this area that has immense growth potential for the future, and even Google‚Äôs Chief Economist thinks the world needs more data scientists.\nSee if this catches your attention, and don‚Äôt think that just because you don‚Äôt know what Hessians are, you can‚Äôt get into Data Science.\nI didn‚Äôt intend to make this a guide for economists on how to become data scientists. But it should possibly give you a lot of things to think about ‚Äî and expand your range of possible career options. Keep an eye on the blog, as I will always be posting about this type of subject. In future publications I will be posting small applied exercises, using R or Python to awaken the reader‚Äôs curiosity to learn.\n\n\n\n\n\n\nHey! üëã, did you find my work useful? Consider buying me a coffee ‚òï, by clicking here üëáüèª"
  },
  {
    "objectID": "consultancy/index.html",
    "href": "consultancy/index.html",
    "title": "Consultancy ",
    "section": "",
    "text": "Consultancy \nI am open, mainly, to consultancy for risk assessment (methods and models).\nIf you want to make sure you are using your data (work or research) to its full potential, please contact me! I have an M.Sc. in applied economics, I have applied methods and models in all my professional experiences, I have spent more than 500 hours teaching university students (as a TA and monograph advisor in an MBA in Data Science & Analytics) and my friends You can testify that I can talk about R effortlessly for days!\nIf you want to hire me, here is a short list of things I can do for you:\n\nResearch project\nStatistical inference\nMachine Learning\nData visualization\nScientific communication\nFinancial & Non-Financial Risk Management\n\nPlease use this form for all enquiries about consultancy, collaborations, or speaking engagements.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFull Name \nEmail Address \nMessage\n\n\nSend message"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Henrique Costa",
    "section": "",
    "text": "M.Sc. Applied Economics\nFederal University of Mato Grosso (Brazil) 2017 ‚Äî 2019\n\nB.A. Economics\nFederal University of Mato Grosso (Brazil) 2013 ‚Äî 2017"
  },
  {
    "objectID": "cv/index.html#education",
    "href": "cv/index.html#education",
    "title": "Henrique Costa",
    "section": "",
    "text": "M.Sc. Applied Economics\nFederal University of Mato Grosso (Brazil) 2017 ‚Äî 2019\n\nB.A. Economics\nFederal University of Mato Grosso (Brazil) 2013 ‚Äî 2017"
  },
  {
    "objectID": "cv/index.html#selected-work-experience",
    "href": "cv/index.html#selected-work-experience",
    "title": "Henrique Costa",
    "section": "Selected Work Experience ",
    "text": "Selected Work Experience \n\nSenior Credit and Market Risk Analyst\nCredsystem, S√£o Paulo, SP - Brazil Jun 2024 ‚Äî Present\nProviding:\n\nIntegrated Risks\nPolicies and Model Validation\nRegulatory Risk - Prudential\nStress Test Program\nCredit Risk modeling (Expected Losses models)\nIFRS (9 & 17) modeling\nCMN/BCB 4.966-21 & BCB 352-23\nMarket Risk\n\n\\(\\Delta\\) NII and \\(\\Delta\\) EVE\nRisk monitoring of market exposures (Value at Risk, VaR Stressed, CVaR-ES);\n\nEnterprise Risk\n\n\n\nStrategy, Credit Risk and Profitability\nQuintoAndar, S√£o Paulo, SP - Brazil Oct 2022 ‚Äî Jun 2024\nProviding:\n\nProfitability modeling (NPV)\nCredit Risk modeling (Expected Losses models)\nIFRS (9 & 17) modeling\nEnterprise Risk\n\n\n\nAcademic Advisor\nMBA in Data Science & Analytics - USP/Esalq, S√£o Paulo, SP - Brazil Sept 2021 ‚Äî April 2023\nMonograph advisor:\n\nCourse Conclusion Paper Guidance for MBA students in Data Science & Analytics.\n\n\n\nFinancial and Regulatory Risk\nCrefisa Bank, S√£o Paulo, SP - Brazil Feb 2022 ‚Äî Oct 2022\nProviding:\n\nRisk and Capital Management (BACEN based on the Basel agreement) for S4\nRisk monitoring of market exposures (Value at Risk, VaR Stressed, CVaR-ES)\nFinancial Risk Management regulatory reports:\n\nDaily Risk Statement - DDR,\nMarket Risk Statement - DRM,\nLiquidity Risk Statement - DRL,\nStatement of Operational Limits - DLO and\nStatement of Individual Limits - DLI.\n\n\n\n\nRisk Analyst in Structured Finance\nLiberum Ratings, S√£o Paulo, SP - Brazil Jan 2021 ‚Äî Feb 2022\nProviding:\n\nRisk assessment in Structured Finance (Specialized in Credit Rights Investment Funds - FIDC)\nAssignment and monitoring of risk classifications (ratings)\nMarket risk assessments (macroeconomic)\nAssessments of sectoral (microeconomic) risks\n\n\n\nRetirement Fund Risk Analyst\nAgenda Assessoria (Consultancy), Cuiab√°, MT - Brazil Jan 2020 ‚Äî Jan 2021\nProviding:\n\nAssessment of financial risks in investment fund portfolios\nDevelopment of investment policies for retirement funds\nMonitoring of Economic Indicators\nStress scenarios based on Foward-looking analysis\nTechnical report of investment portfolio for Own Social Security System - RPPS (Retirement Fund Risk Assessments)\nMarket risk assessments (macroeconomic)\nAssessments of sectoral (microeconomic) risks\n\n\n\nResearcher and Data Analyst (Consultancy)\nFiemt - Federation of Industries in the State of Mato Grosso, Cuiab√°, MT - Brazil ‚Äî\nProviding:\n\nData wrangling process for industrial sector data.\nBuilding dashboards (PowerBI)\nMacroeconomic reports\n\n\n\nCorporate Credit Analyst\nGerencial Factoring Fomento Mercantil, Cuiab√°, MT - Brazil Jan 2018 ‚Äî Oct 2019\nProviding:\n\nCredit and corporate risk analysis for credit assignment.\nOperations for acquiring credit rights (anticipation of receivables).\n\n\n\nAccounting and Finance Department Intern\nMato Grosso Cotton Growers Association - AMPA, Cuiab√°, MT - Brazil Dec 2015 ‚Äî Dec 2016\n\nIntern, in the Accounting and Financial Control Department of the Mato Grosso Association of Cotton Producers (AMPA - Algod√£o de MT).\n\n\n\nTeaching Assistant - Econometrics\nFederal University of Mato Grosso (Brazil) July 2014 ‚Äî Jan 2019\n\n(When I was in undergraduate) Helping undergraduate students to understand the theoretical and practical basis for solving homework assignments, in addition to application in R language\nSupport for students who participated in the Introductory Data Science courses in R Language\n(When I was in my master‚Äôs degree) Assisting the econometrics teacher in introductory classes, applying and correcting econometrics tests, in addition to providing support with homework\n\n\n\nResearch Assistant\nFederal University of Mato Grosso (Brazil) Jan 2014 ‚Äî Dec 2015\n\nParticipating student, researcher assistant, (execution, economic statistics, production and market analysis) in the research project, Santander Universidade Solid√°ria - Sustainable Development in the Pantaneira Hydrographic Basin: implementation of agroecological practices in Cooperangi - Pocon√© - MT, Banco Santander."
  }
]